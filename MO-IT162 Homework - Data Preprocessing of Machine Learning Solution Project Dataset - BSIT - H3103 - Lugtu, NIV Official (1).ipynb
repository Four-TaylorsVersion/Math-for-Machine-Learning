{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a38f5f0",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Machine Learning Solution Project Dataset\n",
    "\n",
    "This notebook cleans and standardizes **three related datasets**:\n",
    "\n",
    "- **Demographics** (`customer_demographics_contaminated (1).csv`): CustomerID, Age, Gender, Location, IncomeLevel, SignupDate\n",
    "- **Transactions** (`customer_transactions_contaminated.csv`): CustomerID, TransactionID, TransactionDate, Amount, ProductCategory, PaymentMethod\n",
    "- **Social Media Interactions** (`social_media_interactions_contaminated (1).csv`): CustomerID, InteractionID, InteractionDate, Platform, InteractionType, Sentiment\n",
    "\n",
    "You'll find each step explained in **Markdown** (why the step matters) followed by **executable Python code** with comments.\n",
    "\n",
    "**High-level goals:**  \n",
    "- Ensure consistent column names and data types  \n",
    "- Handle missing values, duplicates, outliers  \n",
    "- Normalize categories (e.g., gender, platforms, payment methods)  \n",
    "- Parse dates and numeric amounts robustly  \n",
    "- Save **cleaned** CSVs plus a **joined master** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565aaed",
   "metadata": {},
   "source": [
    "## STEP 1 — Loading the CSVs\n",
    "In this step, the three raw CSV datasets are imported into pandas DataFrames. Displaying their shapes and initial rows allows us to verify that the files were successfully loaded and to obtain an initial view of their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc720fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics dataset shape: (3200, 6)\n",
      "Transactions dataset shape: (3200, 6)\n",
      "Social Media dataset shape: (3200, 6)\n",
      "\n",
      "=== DEMOGRAPHICS (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Location</th>\n",
       "      <th>IncomeLevel</th>\n",
       "      <th>SignupDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9207fa75-5758-48d1-94ad-19c041e0520f</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jensenberg</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-11-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fb09cd8-a473-46f7-80bd-6e49cf509078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>Castilloport</td>\n",
       "      <td>High</td>\n",
       "      <td>2020-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c139496e-cc89-498a-bd90-1fb4627b6cff</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Lake Jennifertown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50118139-7264-428f-81cc-a25fddc5d6dd</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Carl</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2024-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jessebury</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-08-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID   Age  Gender           Location  \\\n",
       "0  9207fa75-5758-48d1-94ad-19c041e0520f  51.0  Female         Jensenberg   \n",
       "1  5fb09cd8-a473-46f7-80bd-6e49cf509078   NaN  Female       Castilloport   \n",
       "2  c139496e-cc89-498a-bd90-1fb4627b6cff  37.0    Male  Lake Jennifertown   \n",
       "3  50118139-7264-428f-81cc-a25fddc5d6dd  44.0    Male          Port Carl   \n",
       "4  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4  50.0  Female          Jessebury   \n",
       "\n",
       "  IncomeLevel  SignupDate  \n",
       "0         Low  2022-11-17  \n",
       "1        High  2020-07-21  \n",
       "2         NaN  2021-01-01  \n",
       "3      Medium  2024-06-10  \n",
       "4        High  2023-08-24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSACTIONS (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>Amount</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>PaymentMethod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60567026-f719-4cd6-849e-137e86d8938f</td>\n",
       "      <td>5ff75116-0a50-4d04-80fb-31e5ccbb0769</td>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>117.64</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>PayPal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4090ba85-b111-4f75-a792-c777965f5255</td>\n",
       "      <td>2c39b9fe-ff57-4d39-9321-9f5cdf187aa1</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>466.14</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9223891b-73ff-4d5c-b8ae-13ece82ee28b</td>\n",
       "      <td>f79588dd-3db9-4ffa-97f8-7de0e64259f1</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>563.99</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Debit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9243eebc-938f-480c-8564-16d503d250de</td>\n",
       "      <td>401c0fc9-60df-4455-ad78-67c132f9897d</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>254.44</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>PayPal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f</td>\n",
       "      <td>2034aebc-8280-4254-a667-92bcd1c2be4f</td>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>590.52</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID                         TransactionID  \\\n",
       "0  60567026-f719-4cd6-849e-137e86d8938f  5ff75116-0a50-4d04-80fb-31e5ccbb0769   \n",
       "1  4090ba85-b111-4f75-a792-c777965f5255  2c39b9fe-ff57-4d39-9321-9f5cdf187aa1   \n",
       "2  9223891b-73ff-4d5c-b8ae-13ece82ee28b  f79588dd-3db9-4ffa-97f8-7de0e64259f1   \n",
       "3  9243eebc-938f-480c-8564-16d503d250de  401c0fc9-60df-4455-ad78-67c132f9897d   \n",
       "4  6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f  2034aebc-8280-4254-a667-92bcd1c2be4f   \n",
       "\n",
       "  TransactionDate  Amount  ProductCategory  PaymentMethod  \n",
       "0      2024-05-15  117.64         Clothing         PayPal  \n",
       "1      2023-04-26  466.14  Health & Beauty  Bank Transfer  \n",
       "2      2022-09-23  563.99         Clothing     Debit Card  \n",
       "3      2024-04-15  254.44       Automotive         PayPal  \n",
       "4      2024-06-03  590.52    Home & Garden  Bank Transfer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SOCIAL MEDIA (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>InteractionID</th>\n",
       "      <th>InteractionDate</th>\n",
       "      <th>Platform</th>\n",
       "      <th>InteractionType</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2dcb9523-356b-40b2-a67b-1f27797de261</td>\n",
       "      <td>e5d15761-d0a7-4329-89e3-79a892c56097</td>\n",
       "      <td>2023-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comment</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e12c37b3-7d4d-472f-9fd8-0df2cb3001aa</td>\n",
       "      <td>02f9f376-70ae-4fcd-9070-1db977939948</td>\n",
       "      <td>2023-07-06</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Share</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2</td>\n",
       "      <td>a83fa04c-f109-4f24-8ce1-2078154f6a1c</td>\n",
       "      <td>2024-05-24</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efdfdfc9-5dbb-4478-911a-101a390a0285</td>\n",
       "      <td>28a69c4b-a2e4-4c74-a130-1132d7733fdf</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Like</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca1e90f6-0e5f-492e-ab92-252ff540da18</td>\n",
       "      <td>d9d1c6f8-5e15-4738-b52b-13c2982420cc</td>\n",
       "      <td>2023-07-08</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Like</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID                         InteractionID  \\\n",
       "0  2dcb9523-356b-40b2-a67b-1f27797de261  e5d15761-d0a7-4329-89e3-79a892c56097   \n",
       "1  e12c37b3-7d4d-472f-9fd8-0df2cb3001aa  02f9f376-70ae-4fcd-9070-1db977939948   \n",
       "2  08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2  a83fa04c-f109-4f24-8ce1-2078154f6a1c   \n",
       "3  efdfdfc9-5dbb-4478-911a-101a390a0285  28a69c4b-a2e4-4c74-a130-1132d7733fdf   \n",
       "4  ca1e90f6-0e5f-492e-ab92-252ff540da18  d9d1c6f8-5e15-4738-b52b-13c2982420cc   \n",
       "\n",
       "  InteractionDate   Platform InteractionType Sentiment  \n",
       "0      2023-07-11        NaN         Comment       NaN  \n",
       "1      2023-07-06    Twitter           Share       NaN  \n",
       "2      2024-05-24  Instagram         Comment   Neutral  \n",
       "3      2023-11-01  Instagram            Like   Neutral  \n",
       "4      2023-07-08  Instagram            Like       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Load the CSV files into pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load each dataset into a DataFrame\n",
    "demo_raw   = pd.read_csv(\"customer_demographics_contaminated (1).csv\")      # Customer demographic information\n",
    "txn_raw    = pd.read_csv(\"customer_transactions_contaminated.csv\")          # Customer transaction history\n",
    "social_raw = pd.read_csv(\"social_media_interactions_contaminated (1).csv\")  # Social media interactions\n",
    "\n",
    "# Display the shape of each DataFrame (rows, columns) to confirm successful loading\n",
    "print(\"Demographics dataset shape:\", demo_raw.shape)\n",
    "print(\"Transactions dataset shape:\", txn_raw.shape)\n",
    "print(\"Social Media dataset shape:\", social_raw.shape)\n",
    "\n",
    "# Preview the first five rows of each dataset to examine their structure and values\n",
    "print(\"\\n=== DEMOGRAPHICS (first 5 rows) ===\")\n",
    "display(demo_raw.head())\n",
    "\n",
    "print(\"\\n=== TRANSACTIONS (first 5 rows) ===\")\n",
    "display(txn_raw.head())\n",
    "\n",
    "print(\"\\n=== SOCIAL MEDIA (first 5 rows) ===\")\n",
    "display(social_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464db5",
   "metadata": {},
   "source": [
    "## STEP 2 — Quick Checks (columns, types, missing values)\n",
    "This step examines the basic structure of each dataset (column names, data types, and missing values). These diagnostics inform the subsequent cleaning operations such as type parsing, imputation, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad06e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics columns: ['CustomerID', 'Age', 'Gender', 'Location', 'IncomeLevel', 'SignupDate']\n",
      "Transactions columns: ['CustomerID', 'TransactionID', 'TransactionDate', 'Amount', 'ProductCategory', 'PaymentMethod']\n",
      "Social columns: ['CustomerID', 'InteractionID', 'InteractionDate', 'Platform', 'InteractionType', 'Sentiment']\n",
      "\n",
      "Demographics dtypes:\n",
      " CustomerID     object\n",
      "Age            object\n",
      "Gender         object\n",
      "Location       object\n",
      "IncomeLevel    object\n",
      "SignupDate     object\n",
      "dtype: object\n",
      "\n",
      "Transactions dtypes:\n",
      " CustomerID         object\n",
      "TransactionID      object\n",
      "TransactionDate    object\n",
      "Amount             object\n",
      "ProductCategory    object\n",
      "PaymentMethod      object\n",
      "dtype: object\n",
      "\n",
      "Social dtypes:\n",
      " CustomerID         object\n",
      "InteractionID      object\n",
      "InteractionDate    object\n",
      "Platform           object\n",
      "InteractionType    object\n",
      "Sentiment          object\n",
      "dtype: object\n",
      "\n",
      "Missing values (Demographics):\n",
      " CustomerID       0\n",
      "Age            291\n",
      "Gender           0\n",
      "Location         0\n",
      "IncomeLevel    303\n",
      "SignupDate       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (Transactions):\n",
      " CustomerID           0\n",
      "TransactionID        0\n",
      "TransactionDate      0\n",
      "Amount             304\n",
      "ProductCategory    299\n",
      "PaymentMethod        0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (Social):\n",
      " CustomerID           0\n",
      "InteractionID        0\n",
      "InteractionDate      0\n",
      "Platform           311\n",
      "InteractionType      0\n",
      "Sentiment          329\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Structural checks (columns, types, missing values)\n",
    "\n",
    "# Column names\n",
    "print(\"Demographics columns:\", list(demo_raw.columns))\n",
    "print(\"Transactions columns:\", list(txn_raw.columns))\n",
    "print(\"Social columns:\", list(social_raw.columns))\n",
    "\n",
    "# Data types\n",
    "print(\"\\nDemographics dtypes:\\n\", demo_raw.dtypes)\n",
    "print(\"\\nTransactions dtypes:\\n\", txn_raw.dtypes)\n",
    "print(\"\\nSocial dtypes:\\n\", social_raw.dtypes)\n",
    "\n",
    "# Missing values per column\n",
    "print(\"\\nMissing values (Demographics):\\n\", demo_raw.isna().sum())\n",
    "print(\"\\nMissing values (Transactions):\\n\", txn_raw.isna().sum())\n",
    "print(\"\\nMissing values (Social):\\n\", social_raw.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3591d",
   "metadata": {},
   "source": [
    "## STEP 3 — Standardize Column Names\n",
    "Column names are standardized to a consistent snake_case convention. Consistent naming reduces the likelihood of downstream errors and simplifies reference in code and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2da1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'age', 'gender', 'location', 'income_level', 'signup_date']\n",
      "['customer_id', 'transaction_id', 'transaction_date', 'amount', 'product_category', 'payment_method']\n",
      "['customer_id', 'interaction_id', 'interaction_date', 'platform', 'interaction_type', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Standardize column names to snake_case\n",
    "\n",
    "# Create normalized copies\n",
    "demo = demo_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"Age\": \"age\",\n",
    "    \"Gender\": \"gender\",\n",
    "    \"Location\": \"location\",\n",
    "    \"IncomeLevel\": \"income_level\",\n",
    "    \"SignupDate\": \"signup_date\"\n",
    "})\n",
    "\n",
    "txn = txn_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"TransactionID\": \"transaction_id\",\n",
    "    \"TransactionDate\": \"transaction_date\",\n",
    "    \"Amount\": \"amount\",\n",
    "    \"ProductCategory\": \"product_category\",\n",
    "    \"PaymentMethod\": \"payment_method\"\n",
    "})\n",
    "\n",
    "social = social_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"InteractionID\": \"interaction_id\",\n",
    "    \"InteractionDate\": \"interaction_date\",\n",
    "    \"Platform\": \"platform\",\n",
    "    \"InteractionType\": \"interaction_type\",\n",
    "    \"Sentiment\": \"sentiment\"\n",
    "})\n",
    "\n",
    "# Verify standardized names\n",
    "print(list(demo.columns))\n",
    "print(list(txn.columns))\n",
    "print(list(social.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bd098",
   "metadata": {},
   "source": [
    "## STEP 4 — Clean Text Fields and Harmonize Null Tokens\n",
    "Text fields are trimmed to remove extraneous whitespace, and common null tokens (e.g., “NA”, “none”, “-”) are converted to NaN. This ensures that missingness is represented uniformly across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8006eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo dataset (first 10 rows after cleaning):\n",
      "                            customer_id   age  gender           location  \\\n",
      "0  9207fa75-5758-48d1-94ad-19c041e0520f  51.0  Female         Jensenberg   \n",
      "1  5fb09cd8-a473-46f7-80bd-6e49cf509078   NaN  Female       Castilloport   \n",
      "2  c139496e-cc89-498a-bd90-1fb4627b6cff  37.0    Male  Lake Jennifertown   \n",
      "3  50118139-7264-428f-81cc-a25fddc5d6dd  44.0    Male          Port Carl   \n",
      "4  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4  50.0  Female          Jessebury   \n",
      "5  2de49c7c-32ae-4ba8-b058-622a090d7094  53.0  Female         Emilyville   \n",
      "6  89f7de44-e592-43c6-b033-bcbcf24088ba   NaN  Female    South Derekbury   \n",
      "7  9f2128cd-2f2b-4f71-987c-29fd045021f0   NaN  Female          Lake Erin   \n",
      "8  fbcd7128-ce04-4799-8c1c-722330507b96   NaN  Female     New Reginabury   \n",
      "9  40f5a5dd-a46f-4a5e-8c0f-fe04389ddec8   NaN  Female          Mariaberg   \n",
      "\n",
      "  income_level signup_date  \n",
      "0          Low  2022-11-17  \n",
      "1         High  2020-07-21  \n",
      "2          NaN  2021-01-01  \n",
      "3       Medium  2024-06-10  \n",
      "4         High  2023-08-24  \n",
      "5          Low  2022-02-13  \n",
      "6         High  2019-12-08  \n",
      "7       Medium  2022-04-26  \n",
      "8          NaN  2022-04-17  \n",
      "9         High  2024-02-18  \n",
      "\n",
      "Transaction dataset (first 10 rows after cleaning):\n",
      "                            customer_id                        transaction_id  \\\n",
      "0  60567026-f719-4cd6-849e-137e86d8938f  5ff75116-0a50-4d04-80fb-31e5ccbb0769   \n",
      "1  4090ba85-b111-4f75-a792-c777965f5255  2c39b9fe-ff57-4d39-9321-9f5cdf187aa1   \n",
      "2  9223891b-73ff-4d5c-b8ae-13ece82ee28b  f79588dd-3db9-4ffa-97f8-7de0e64259f1   \n",
      "3  9243eebc-938f-480c-8564-16d503d250de  401c0fc9-60df-4455-ad78-67c132f9897d   \n",
      "4  6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f  2034aebc-8280-4254-a667-92bcd1c2be4f   \n",
      "5  3eccdcf9-e8ce-4e44-9f62-8bef88e70672  78dfb45e-16e0-4ecc-8f67-f2e210c31c3e   \n",
      "6  7fbc5847-a652-4fc8-a7d8-5e85d846e91b  591f1ea2-c97d-44dd-9539-71ac89b94ff3   \n",
      "7  958e5c8d-48ca-42dd-bb71-a766a374233a  fb24e098-3ab9-40a2-bcc3-b8ebb23f549a   \n",
      "8  39c6e7d2-6c4b-44c0-8961-ddc1ecbdb0c6  833b026a-7c02-4101-832d-62c07569b0f6   \n",
      "9  474f9233-0616-431b-b3e9-a6feabe68abb  f8bde21a-7a6b-41f6-ac39-eeb77640fa9b   \n",
      "\n",
      "  transaction_date  amount product_category payment_method  \n",
      "0       2024-05-15  117.64         Clothing         PayPal  \n",
      "1       2023-04-26  466.14  Health & Beauty  Bank Transfer  \n",
      "2       2022-09-23  563.99         Clothing     Debit Card  \n",
      "3       2024-04-15  254.44       Automotive         PayPal  \n",
      "4       2024-06-03  590.52    Home & Garden  Bank Transfer  \n",
      "5       2024-04-07    Free              NaN    Credit Card  \n",
      "6       2024-01-12     NaN       Automotive         PayPal  \n",
      "7       2023-03-10   399.7    Home & Garden         PayPal  \n",
      "8       2024-01-26  296.99         Clothing         PayPal  \n",
      "9       2023-06-15  149.39         Clothing     Debit Card  \n",
      "\n",
      "Social dataset (first 10 rows after cleaning):\n",
      "                            customer_id                        interaction_id  \\\n",
      "0  2dcb9523-356b-40b2-a67b-1f27797de261  e5d15761-d0a7-4329-89e3-79a892c56097   \n",
      "1  e12c37b3-7d4d-472f-9fd8-0df2cb3001aa  02f9f376-70ae-4fcd-9070-1db977939948   \n",
      "2  08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2  a83fa04c-f109-4f24-8ce1-2078154f6a1c   \n",
      "3  efdfdfc9-5dbb-4478-911a-101a390a0285  28a69c4b-a2e4-4c74-a130-1132d7733fdf   \n",
      "4  ca1e90f6-0e5f-492e-ab92-252ff540da18  d9d1c6f8-5e15-4738-b52b-13c2982420cc   \n",
      "5  3e44871b-f56c-4576-b1ca-d1dc999e2166  0c409883-8396-48e4-83fb-887329848696   \n",
      "6  aa5eea4b-c948-41f4-9285-229a470002aa  4034dadf-6541-40d6-a7f0-16b20a009c04   \n",
      "7  7d83304d-2501-4c9f-ba63-a1a14343e51f  bcfc43e7-c5aa-4dbd-9961-0e108784b199   \n",
      "8  bd90f5cb-05a7-40f4-acb1-eedf48b58ffa  d2a06664-703d-4bc1-9401-a06f8c43fda5   \n",
      "9  63ee220c-19ae-4113-b45c-276b22b068e1  67cdfb0b-3da6-46b8-a1f4-03b2abc81f58   \n",
      "\n",
      "  interaction_date   platform interaction_type sentiment  \n",
      "0       2023-07-11        NaN          Comment       NaN  \n",
      "1       2023-07-06    Twitter            Share       NaN  \n",
      "2       2024-05-24  Instagram          Comment   Neutral  \n",
      "3       2023-11-01  Instagram             Like   Neutral  \n",
      "4       2023-07-08  Instagram             Like       NaN  \n",
      "5       2023-12-18  Instagram          Comment  Positive  \n",
      "6       2023-11-15  Instagram            Share  Positive  \n",
      "7       2024-03-29  Instagram          Comment       NaN  \n",
      "8       2024-05-02        NaN             Like   Neutral  \n",
      "9       2023-11-18        NaN          Comment  Positive  \n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Trim whitespace and convert common null tokens to NaN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define tokens that should be treated as missing values\n",
    "null_tokens = {\"\", \"na\", \"n/a\", \"none\", \"null\", \"nan\", \"-\", \"--\", \"unknown\", \"missing\"}\n",
    "\n",
    "# Apply to all object (string) columns in each dataset\n",
    "for df in [demo, txn, social]:\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        # Strip leading/trailing whitespace\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        # Replace listed tokens with NaN\n",
    "        df[col] = df[col].replace({t: np.nan for t in null_tokens})\n",
    "        # Convert literal strings \"nan\"/\"None\" to NaN\n",
    "        df[col] = df[col].replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "# Print first 10 rows after cleaning\n",
    "print(\"Demo dataset (first 10 rows after cleaning):\")\n",
    "print(demo.head(10))\n",
    "\n",
    "print(\"\\nTransaction dataset (first 10 rows after cleaning):\")\n",
    "print(txn.head(10))\n",
    "\n",
    "print(\"\\nSocial dataset (first 10 rows after cleaning):\")\n",
    "print(social.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c0f4d",
   "metadata": {},
   "source": [
    "## STEP 5 — Remove Exact Duplicate Rows\n",
    "Exact duplicate rows are removed to avoid double-counting and ensure the reliability of aggregations and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55164c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics duplicates removed: 177\n",
      "Transactions duplicates removed: 185\n",
      "Social duplicates removed: 180\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Drop exact duplicates\n",
    "\n",
    "demo_before = len(demo)\n",
    "demo = demo.drop_duplicates()\n",
    "print(\"Demographics duplicates removed:\", demo_before - len(demo))\n",
    "\n",
    "txn_before = len(txn)\n",
    "txn = txn.drop_duplicates()\n",
    "print(\"Transactions duplicates removed:\", txn_before - len(txn))\n",
    "\n",
    "social_before = len(social)\n",
    "social = social.drop_duplicates()\n",
    "print(\"Social duplicates removed:\", social_before - len(social))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d2fa1",
   "metadata": {},
   "source": [
    "## STEP 6 — Parse Dates and Coerce Numeric Amounts\n",
    "Date columns are converted into proper datetime format, and the amount column is coerced into numeric values. Using .loc ensures we avoid chained assignment warnings in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf583c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amount\n",
      "0  117.64\n",
      "1  466.14\n",
      "2  563.99\n",
      "3  254.44\n",
      "4  590.52\n",
      "\n",
      "First 10 signup_date entries:\n",
      "0    2022-11-17 00:00:00\n",
      "1    2020-07-21 00:00:00\n",
      "2    2021-01-01 00:00:00\n",
      "3    2024-06-10 00:00:00\n",
      "4    2023-08-24 00:00:00\n",
      "5    2022-02-13 00:00:00\n",
      "6    2019-12-08 00:00:00\n",
      "7    2022-04-26 00:00:00\n",
      "8    2022-04-17 00:00:00\n",
      "9    2024-02-18 00:00:00\n",
      "Name: signup_date, dtype: object\n",
      "\n",
      "First 10 transaction_date entries:\n",
      "0    2024-05-15 00:00:00\n",
      "1    2023-04-26 00:00:00\n",
      "2    2022-09-23 00:00:00\n",
      "3    2024-04-15 00:00:00\n",
      "4    2024-06-03 00:00:00\n",
      "5    2024-04-07 00:00:00\n",
      "6    2024-01-12 00:00:00\n",
      "7    2023-03-10 00:00:00\n",
      "8    2024-01-26 00:00:00\n",
      "9    2023-06-15 00:00:00\n",
      "Name: transaction_date, dtype: object\n",
      "\n",
      "First 10 interaction_date entries:\n",
      "0    2023-07-11 00:00:00\n",
      "1    2023-07-06 00:00:00\n",
      "2    2024-05-24 00:00:00\n",
      "3    2023-11-01 00:00:00\n",
      "4    2023-07-08 00:00:00\n",
      "5    2023-12-18 00:00:00\n",
      "6    2023-11-15 00:00:00\n",
      "7    2024-03-29 00:00:00\n",
      "8    2024-05-02 00:00:00\n",
      "9    2023-11-18 00:00:00\n",
      "Name: interaction_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Parse dates and convert amount to numeric (revised)\n",
    "\n",
    "# Convert date-like columns safely using .loc\n",
    "if \"signup_date\" in demo.columns:\n",
    "    demo.loc[:, \"signup_date\"] = pd.to_datetime(demo[\"signup_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"transaction_date\" in txn.columns:\n",
    "    txn.loc[:, \"transaction_date\"] = pd.to_datetime(txn[\"transaction_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"interaction_date\" in social.columns:\n",
    "    social.loc[:, \"interaction_date\"] = pd.to_datetime(social[\"interaction_date\"], errors=\"coerce\")\n",
    "\n",
    "# Safely handle the 'amount' column\n",
    "if \"amount\" in txn.columns:\n",
    "    # Step 1: work on a temporary string series\n",
    "    amt_str = txn[\"amount\"].astype(str)\n",
    "    # Step 2: remove currency symbols, spaces, and commas\n",
    "    amt_str = (amt_str.str.replace(\"$\", \"\", regex=False)\n",
    "                        .str.replace(\"₱\", \"\", regex=False)\n",
    "                        .str.replace(\",\", \"\", regex=False)\n",
    "                        .str.replace(\" \", \"\", regex=False))\n",
    "    # Step 3: convert cleaned strings to numeric\n",
    "    txn.loc[:, \"amount\"] = pd.to_numeric(amt_str, errors=\"coerce\")\n",
    "\n",
    "# Quick verification\n",
    "print(txn[[\"amount\"]].head())\n",
    "\n",
    "# Print first 10 entries of datetime columns\n",
    "if \"signup_date\" in demo.columns:\n",
    "    print(\"\\nFirst 10 signup_date entries:\")\n",
    "    print(demo[\"signup_date\"].head(10))\n",
    "\n",
    "if \"transaction_date\" in txn.columns:\n",
    "    print(\"\\nFirst 10 transaction_date entries:\")\n",
    "    print(txn[\"transaction_date\"].head(10))\n",
    "\n",
    "if \"interaction_date\" in social.columns:\n",
    "    print(\"\\nFirst 10 interaction_date entries:\")\n",
    "    print(social[\"interaction_date\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce10040",
   "metadata": {},
   "source": [
    "## STEP 7 — Normalize Categorical Values\n",
    "Categorical values (e.g., gender, payment method, product category, platform, sentiment) are normalized to consistent labels. Assignments use .loc to avoid chained-assignment warnings and ensure changes apply to the intended DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627ea916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defensive copy to avoid any view-related warnings\n",
    "demo   = demo.copy()\n",
    "txn    = txn.copy()\n",
    "social = social.copy()\n",
    "\n",
    "# --- Gender (demographics) ---\n",
    "def normalize_gender(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"m\", \"male\"]:\n",
    "        return \"Male\"\n",
    "    if s in [\"f\", \"female\"]:\n",
    "        return \"Female\"\n",
    "    if s in [\"other\", \"nonbinary\", \"non-binary\", \"nb\"]:\n",
    "        return \"Other\"\n",
    "    return str(x).title()\n",
    "\n",
    "if \"gender\" in demo.columns:\n",
    "    demo.loc[:, \"gender\"] = demo[\"gender\"].map(normalize_gender)\n",
    "\n",
    "# --- Payment method & product category (transactions) ---\n",
    "if \"payment_method\" in txn.columns:\n",
    "    txn.loc[:, \"payment_method\"] = (\n",
    "        txn[\"payment_method\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "if \"product_category\" in txn.columns:\n",
    "    txn.loc[:, \"product_category\"] = (\n",
    "        txn[\"product_category\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "# --- Platform & interaction type (social) ---\n",
    "if \"platform\" in social.columns:\n",
    "    social.loc[:, \"platform\"] = (\n",
    "        social[\"platform\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "if \"interaction_type\" in social.columns:\n",
    "    social.loc[:, \"interaction_type\"] = (\n",
    "        social[\"interaction_type\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "# --- Sentiment (social) ---\n",
    "def normalize_sentiment(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"pos\", \"positive\", \"1\", \"+\", \"good\"]:\n",
    "        return \"Positive\"\n",
    "    if s in [\"neu\", \"neutral\", \"0\", \"meh\"]:\n",
    "        return \"Neutral\"\n",
    "    if s in [\"neg\", \"negative\", \"-1\", \"-\", \"bad\"]:\n",
    "        return \"Negative\"\n",
    "    return str(x).title()\n",
    "\n",
    "if \"sentiment\" in social.columns:\n",
    "    social.loc[:, \"sentiment\"] = social[\"sentiment\"].map(normalize_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb85ad",
   "metadata": {},
   "source": [
    "## STEP 8 — Handle Missing Values\n",
    "Missing values are addressed by dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a863923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs (Demographics):\n",
      " customer_id     0\n",
      "age             0\n",
      "gender          0\n",
      "location        0\n",
      "income_level    0\n",
      "signup_date     0\n",
      "dtype: int64\n",
      "\n",
      "Remaining NaNs (Transactions):\n",
      " customer_id         0\n",
      "transaction_id      0\n",
      "transaction_date    0\n",
      "amount              0\n",
      "product_category    0\n",
      "payment_method      0\n",
      "is_refund           0\n",
      "dtype: int64\n",
      "\n",
      "Remaining NaNs (Social):\n",
      " customer_id         0\n",
      "interaction_id      0\n",
      "interaction_date    0\n",
      "platform            0\n",
      "interaction_type    0\n",
      "sentiment           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make sure numeric columns are coerced properly\n",
    "if \"age\" in demo.columns:\n",
    "    demo[\"age\"] = pd.to_numeric(demo[\"age\"], errors=\"coerce\")\n",
    "\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"amount\"] = pd.to_numeric(txn[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing essential fields\n",
    "demo = demo.dropna(subset=[\"customer_id\", \"age\", \"gender\", \"location\"])\n",
    "txn = txn.dropna(subset=[\"customer_id\", \"transaction_id\", \"amount\"])\n",
    "social = social.dropna(subset=[\"customer_id\", \"interaction_id\", \"interaction_date\"])\n",
    "\n",
    "# Recreate refund flag\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"is_refund\"] = txn[\"amount\"] < 0\n",
    "    \n",
    "demo = demo.dropna().copy()\n",
    "txn = txn.dropna().copy()\n",
    "social = social.dropna().copy()\n",
    "\n",
    "# Recreate refund flag (in case df changed)\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"is_refund\"] = txn[\"amount\"] < 0\n",
    "\n",
    "# Re-check\n",
    "print(\"Remaining NaNs (Demographics):\\n\", demo.isna().sum())\n",
    "print(\"\\nRemaining NaNs (Transactions):\\n\", txn.isna().sum())\n",
    "print(\"\\nRemaining NaNs (Social):\\n\", social.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e85cf",
   "metadata": {},
   "source": [
    "## STEP 9 — Mitigate Outliers via Light Clipping\n",
    "Extreme outliers in numeric fields can distort statistical summaries and models. Here, values are clipped to the 1st and 99th percentiles for age and amount to reduce undue influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b4e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2389.000000\n",
      "mean       45.492256\n",
      "std        19.357553\n",
      "min        -1.000000\n",
      "25%        31.000000\n",
      "50%        45.000000\n",
      "75%        59.000000\n",
      "max       150.000000\n",
      "Name: age, dtype: float64\n",
      "count    2614.000000\n",
      "mean      491.371204\n",
      "std       300.329546\n",
      "min      -100.000000\n",
      "25%       227.130000\n",
      "50%       497.050000\n",
      "75%       748.760000\n",
      "max       992.174000\n",
      "Name: amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Clip age if present\n",
    "if \"age\" in demo.columns:\n",
    "    q1_age  = demo[\"age\"].quantile(0.01)\n",
    "    q99_age = demo[\"age\"].quantile(0.99)\n",
    "    demo[\"age\"] = demo[\"age\"].clip(lower=q1_age, upper=q99_age)\n",
    "\n",
    "# Clip amount if present\n",
    "if \"amount\" in txn.columns:\n",
    "    q1_amt  = txn[\"amount\"].quantile(0.01)\n",
    "    q99_amt = txn[\"amount\"].quantile(0.99)\n",
    "    txn[\"amount\"] = txn[\"amount\"].clip(lower=q1_amt, upper=q99_amt)\n",
    "\n",
    "# Inspect distributions after clipping\n",
    "print(demo[\"age\"].describe() if \"age\" in demo.columns else \"No age column.\")\n",
    "print(txn[\"amount\"].describe() if \"amount\" in txn.columns else \"No amount column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99c3ca",
   "metadata": {},
   "source": [
    "## STEP 10 — Enforce Identifier Uniqueness\n",
    "Primary identifiers are enforced as unique within their respective tables. This step removes any duplicated keys to maintain entity integrity and prevent ambiguity in joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968cfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate customer_id removed (demo): 6\n",
      "Duplicate transaction_id removed (txn): 7\n",
      "Duplicate interaction_id removed (social): 7\n"
     ]
    }
   ],
   "source": [
    "if \"customer_id\" in demo.columns:\n",
    "    before = len(demo)\n",
    "    demo = demo.drop_duplicates(subset=[\"customer_id\"])\n",
    "    print(\"Duplicate customer_id removed (demo):\", before - len(demo))\n",
    "\n",
    "if \"transaction_id\" in txn.columns:\n",
    "    before = len(txn)\n",
    "    txn = txn.drop_duplicates(subset=[\"transaction_id\"])\n",
    "    print(\"Duplicate transaction_id removed (txn):\", before - len(txn))\n",
    "\n",
    "if \"interaction_id\" in social.columns:\n",
    "    before = len(social)\n",
    "    social = social.drop_duplicates(subset=[\"interaction_id\"])\n",
    "    print(\"Duplicate interaction_id removed (social):\", before - len(social))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42e877",
   "metadata": {},
   "source": [
    "## STEP 11 — Build a Customer-Level Master Table\n",
    "A customer-level master table is constructed by left-joining demographic records with aggregated transaction and social metrics. This unified view facilitates downstream analysis, reporting, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70291a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master dataset shape: (2383, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>income_level</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>total_spend</th>\n",
       "      <th>avg_spend</th>\n",
       "      <th>txn_count</th>\n",
       "      <th>refund_count</th>\n",
       "      <th>interactions</th>\n",
       "      <th>platforms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9207fa75-5758-48d1-94ad-19c041e0520f</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jensenberg</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-11-17 00:00:00</td>\n",
       "      <td>660.87</td>\n",
       "      <td>660.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50118139-7264-428f-81cc-a25fddc5d6dd</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Carl</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2024-06-10 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jessebury</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-08-24 00:00:00</td>\n",
       "      <td>733.46</td>\n",
       "      <td>733.46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2de49c7c-32ae-4ba8-b058-622a090d7094</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Emilyville</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-02-13 00:00:00</td>\n",
       "      <td>891.74</td>\n",
       "      <td>445.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c96a5ee9-f1a6-416a-adc6-1c8b128c7399</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hansontown</td>\n",
       "      <td>High</td>\n",
       "      <td>2020-09-17 00:00:00</td>\n",
       "      <td>1184.04</td>\n",
       "      <td>394.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8602d631-457c-49c1-8b59-8efb2a4448d4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Keithville</td>\n",
       "      <td>High</td>\n",
       "      <td>2022-04-17 00:00:00</td>\n",
       "      <td>815.21</td>\n",
       "      <td>815.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56f11a95-76f1-4a97-b38f-db1dc95da1ed</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>East Nathan</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2020-01-02 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3f520998-2bc4-4f38-af82-5ab2de339984</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Smithside</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-10-06 00:00:00</td>\n",
       "      <td>183.91</td>\n",
       "      <td>183.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b2f4c25e-be11-4912-9d14-5c288616e56e</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>South Timothyhaven</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-07-09 00:00:00</td>\n",
       "      <td>533.94</td>\n",
       "      <td>533.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8de7560f-370d-4cfd-8135-20a3de237264</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Colemanstad</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-12-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            customer_id    age  gender            location  \\\n",
       "0  9207fa75-5758-48d1-94ad-19c041e0520f   51.0  Female          Jensenberg   \n",
       "1  50118139-7264-428f-81cc-a25fddc5d6dd   44.0    Male           Port Carl   \n",
       "2  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4   50.0  Female           Jessebury   \n",
       "3  2de49c7c-32ae-4ba8-b058-622a090d7094   53.0  Female          Emilyville   \n",
       "4  c96a5ee9-f1a6-416a-adc6-1c8b128c7399  150.0    Male          Hansontown   \n",
       "5  8602d631-457c-49c1-8b59-8efb2a4448d4   51.0    Male     East Keithville   \n",
       "6  56f11a95-76f1-4a97-b38f-db1dc95da1ed   59.0  Female         East Nathan   \n",
       "7  3f520998-2bc4-4f38-af82-5ab2de339984   59.0  Female           Smithside   \n",
       "8  b2f4c25e-be11-4912-9d14-5c288616e56e   29.0    Male  South Timothyhaven   \n",
       "9  8de7560f-370d-4cfd-8135-20a3de237264   31.0  Female         Colemanstad   \n",
       "\n",
       "  income_level          signup_date  total_spend  avg_spend  txn_count  \\\n",
       "0          Low  2022-11-17 00:00:00       660.87     660.87        1.0   \n",
       "1       Medium  2024-06-10 00:00:00          NaN        NaN        NaN   \n",
       "2         High  2023-08-24 00:00:00       733.46     733.46        1.0   \n",
       "3          Low  2022-02-13 00:00:00       891.74     445.87        2.0   \n",
       "4         High  2020-09-17 00:00:00      1184.04     394.68        3.0   \n",
       "5         High  2022-04-17 00:00:00       815.21     815.21        1.0   \n",
       "6       Medium  2020-01-02 00:00:00          NaN        NaN        NaN   \n",
       "7         High  2023-10-06 00:00:00       183.91     183.91        1.0   \n",
       "8         High  2023-07-09 00:00:00       533.94     533.94        1.0   \n",
       "9          Low  2020-12-01 00:00:00          NaN        NaN        NaN   \n",
       "\n",
       "   refund_count  interactions  platforms  \n",
       "0           0.0           2.0        2.0  \n",
       "1           NaN           2.0        2.0  \n",
       "2           0.0           2.0        2.0  \n",
       "3           0.0           3.0        2.0  \n",
       "4           0.0           NaN        NaN  \n",
       "5           0.0           2.0        2.0  \n",
       "6           NaN           3.0        3.0  \n",
       "7           0.0           NaN        NaN  \n",
       "8           0.0           1.0        1.0  \n",
       "9           NaN           NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregate transactions per customer\n",
    "if set([\"customer_id\", \"amount\", \"transaction_id\"]).issubset(txn.columns):\n",
    "    txn_agg = (txn.groupby(\"customer_id\")\n",
    "                 .agg(total_spend=(\"amount\", \"sum\"),\n",
    "                      avg_spend=(\"amount\", \"mean\"),\n",
    "                      txn_count=(\"transaction_id\", \"nunique\"),\n",
    "                      refund_count=(\"is_refund\", \"sum\"))\n",
    "                 .reset_index())\n",
    "else:\n",
    "    txn_agg = pd.DataFrame(columns=[\"customer_id\", \"total_spend\", \"avg_spend\", \"txn_count\", \"refund_count\"])\n",
    "\n",
    "# Aggregate social interactions per customer\n",
    "if set([\"customer_id\", \"interaction_id\"]).issubset(social.columns):\n",
    "    if \"platform\" in social.columns:\n",
    "        social_agg = (social.groupby(\"customer_id\")\n",
    "                        .agg(interactions=(\"interaction_id\", \"nunique\"),\n",
    "                             platforms=(\"platform\", \"nunique\"))\n",
    "                        .reset_index())\n",
    "    else:\n",
    "        social_agg = (social.groupby(\"customer_id\")\n",
    "                        .agg(interactions=(\"interaction_id\", \"nunique\"))\n",
    "                        .reset_index())\n",
    "else:\n",
    "    social_agg = pd.DataFrame(columns=[\"customer_id\", \"interactions\", \"platforms\"])\n",
    "\n",
    "# Start from demographics (one row per customer), then left-join aggregates\n",
    "master = demo.copy()\n",
    "if not txn_agg.empty:\n",
    "    master = master.merge(txn_agg, on=\"customer_id\", how=\"left\")\n",
    "if not social_agg.empty:\n",
    "    master = master.merge(social_agg, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "print(\"Master dataset shape:\", master.shape)\n",
    "display(master.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366462b4",
   "metadata": {},
   "source": [
    "## STEP 12 — Persist the Cleaned Outputs\n",
    "Finally, the cleaned datasets and the integrated master table are exported to CSV files. Persisting these outputs enables reproducibility and reuse in analytics and BI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd32d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the following files:\n",
      " - cleaned_customer_demographics.csv\n",
      " - cleaned_customer_transactions.csv\n",
      " - cleaned_social_media_interactions.csv\n",
      " - cleaned_master.csv\n"
     ]
    }
   ],
   "source": [
    "demo.to_csv(\"cleaned_customer_demographics.csv\", index=False)\n",
    "txn.to_csv(\"cleaned_customer_transactions.csv\", index=False)\n",
    "social.to_csv(\"cleaned_social_media_interactions.csv\", index=False)\n",
    "master.to_csv(\"cleaned_master.csv\", index=False)\n",
    "\n",
    "print(\"Saved the following files:\")\n",
    "print(\" - cleaned_customer_demographics.csv\")\n",
    "print(\" - cleaned_customer_transactions.csv\")\n",
    "print(\" - cleaned_social_media_interactions.csv\")\n",
    "print(\" - cleaned_master.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
