{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a38f5f0",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Machine Learning Solution Project Dataset\n",
    "\n",
    "This notebook cleans and standardizes **three related datasets**:\n",
    "\n",
    "- **Demographics** (`customer_demographics_contaminated (1).csv`): CustomerID, Age, Gender, Location, IncomeLevel, SignupDate\n",
    "- **Transactions** (`customer_transactions_contaminated.csv`): CustomerID, TransactionID, TransactionDate, Amount, ProductCategory, PaymentMethod\n",
    "- **Social Media Interactions** (`social_media_interactions_contaminated (1).csv`): CustomerID, InteractionID, InteractionDate, Platform, InteractionType, Sentiment\n",
    "\n",
    "You'll find each step explained in **Markdown** (why the step matters) followed by **executable Python code** with comments.\n",
    "\n",
    "**High-level goals:**  \n",
    "- Ensure consistent column names and data types  \n",
    "- Handle missing values, duplicates, outliers  \n",
    "- Normalize categories (e.g., gender, platforms, payment methods)  \n",
    "- Parse dates and numeric amounts robustly  \n",
    "- Save **cleaned** CSVs plus a **joined master** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565aaed",
   "metadata": {},
   "source": [
    "## STEP 1 — Loading the CSVs\n",
    "In this step, the three raw CSV datasets are imported into pandas DataFrames. Displaying their shapes and initial rows allows us to verify that the files were successfully loaded and to obtain an initial view of their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc720fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics dataset shape: (3200, 6)\n",
      "Transactions dataset shape: (3200, 6)\n",
      "Social Media dataset shape: (3200, 6)\n",
      "\n",
      "=== DEMOGRAPHICS (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Location</th>\n",
       "      <th>IncomeLevel</th>\n",
       "      <th>SignupDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9207fa75-5758-48d1-94ad-19c041e0520f</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jensenberg</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-11-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fb09cd8-a473-46f7-80bd-6e49cf509078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>Castilloport</td>\n",
       "      <td>High</td>\n",
       "      <td>2020-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c139496e-cc89-498a-bd90-1fb4627b6cff</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Lake Jennifertown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50118139-7264-428f-81cc-a25fddc5d6dd</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Carl</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2024-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jessebury</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-08-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID   Age  Gender           Location  \\\n",
       "0  9207fa75-5758-48d1-94ad-19c041e0520f  51.0  Female         Jensenberg   \n",
       "1  5fb09cd8-a473-46f7-80bd-6e49cf509078   NaN  Female       Castilloport   \n",
       "2  c139496e-cc89-498a-bd90-1fb4627b6cff  37.0    Male  Lake Jennifertown   \n",
       "3  50118139-7264-428f-81cc-a25fddc5d6dd  44.0    Male          Port Carl   \n",
       "4  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4  50.0  Female          Jessebury   \n",
       "\n",
       "  IncomeLevel  SignupDate  \n",
       "0         Low  2022-11-17  \n",
       "1        High  2020-07-21  \n",
       "2         NaN  2021-01-01  \n",
       "3      Medium  2024-06-10  \n",
       "4        High  2023-08-24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSACTIONS (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>Amount</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>PaymentMethod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60567026-f719-4cd6-849e-137e86d8938f</td>\n",
       "      <td>5ff75116-0a50-4d04-80fb-31e5ccbb0769</td>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>117.64</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>PayPal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4090ba85-b111-4f75-a792-c777965f5255</td>\n",
       "      <td>2c39b9fe-ff57-4d39-9321-9f5cdf187aa1</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>466.14</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9223891b-73ff-4d5c-b8ae-13ece82ee28b</td>\n",
       "      <td>f79588dd-3db9-4ffa-97f8-7de0e64259f1</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>563.99</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Debit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9243eebc-938f-480c-8564-16d503d250de</td>\n",
       "      <td>401c0fc9-60df-4455-ad78-67c132f9897d</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>254.44</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>PayPal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f</td>\n",
       "      <td>2034aebc-8280-4254-a667-92bcd1c2be4f</td>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>590.52</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID                         TransactionID  \\\n",
       "0  60567026-f719-4cd6-849e-137e86d8938f  5ff75116-0a50-4d04-80fb-31e5ccbb0769   \n",
       "1  4090ba85-b111-4f75-a792-c777965f5255  2c39b9fe-ff57-4d39-9321-9f5cdf187aa1   \n",
       "2  9223891b-73ff-4d5c-b8ae-13ece82ee28b  f79588dd-3db9-4ffa-97f8-7de0e64259f1   \n",
       "3  9243eebc-938f-480c-8564-16d503d250de  401c0fc9-60df-4455-ad78-67c132f9897d   \n",
       "4  6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f  2034aebc-8280-4254-a667-92bcd1c2be4f   \n",
       "\n",
       "  TransactionDate  Amount  ProductCategory  PaymentMethod  \n",
       "0      2024-05-15  117.64         Clothing         PayPal  \n",
       "1      2023-04-26  466.14  Health & Beauty  Bank Transfer  \n",
       "2      2022-09-23  563.99         Clothing     Debit Card  \n",
       "3      2024-04-15  254.44       Automotive         PayPal  \n",
       "4      2024-06-03  590.52    Home & Garden  Bank Transfer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SOCIAL MEDIA (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>InteractionID</th>\n",
       "      <th>InteractionDate</th>\n",
       "      <th>Platform</th>\n",
       "      <th>InteractionType</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2dcb9523-356b-40b2-a67b-1f27797de261</td>\n",
       "      <td>e5d15761-d0a7-4329-89e3-79a892c56097</td>\n",
       "      <td>2023-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comment</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e12c37b3-7d4d-472f-9fd8-0df2cb3001aa</td>\n",
       "      <td>02f9f376-70ae-4fcd-9070-1db977939948</td>\n",
       "      <td>2023-07-06</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Share</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2</td>\n",
       "      <td>a83fa04c-f109-4f24-8ce1-2078154f6a1c</td>\n",
       "      <td>2024-05-24</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efdfdfc9-5dbb-4478-911a-101a390a0285</td>\n",
       "      <td>28a69c4b-a2e4-4c74-a130-1132d7733fdf</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Like</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca1e90f6-0e5f-492e-ab92-252ff540da18</td>\n",
       "      <td>d9d1c6f8-5e15-4738-b52b-13c2982420cc</td>\n",
       "      <td>2023-07-08</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Like</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID                         InteractionID  \\\n",
       "0  2dcb9523-356b-40b2-a67b-1f27797de261  e5d15761-d0a7-4329-89e3-79a892c56097   \n",
       "1  e12c37b3-7d4d-472f-9fd8-0df2cb3001aa  02f9f376-70ae-4fcd-9070-1db977939948   \n",
       "2  08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2  a83fa04c-f109-4f24-8ce1-2078154f6a1c   \n",
       "3  efdfdfc9-5dbb-4478-911a-101a390a0285  28a69c4b-a2e4-4c74-a130-1132d7733fdf   \n",
       "4  ca1e90f6-0e5f-492e-ab92-252ff540da18  d9d1c6f8-5e15-4738-b52b-13c2982420cc   \n",
       "\n",
       "  InteractionDate   Platform InteractionType Sentiment  \n",
       "0      2023-07-11        NaN         Comment       NaN  \n",
       "1      2023-07-06    Twitter           Share       NaN  \n",
       "2      2024-05-24  Instagram         Comment   Neutral  \n",
       "3      2023-11-01  Instagram            Like   Neutral  \n",
       "4      2023-07-08  Instagram            Like       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Load the CSV files into pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load each dataset into a DataFrame\n",
    "demo_raw   = pd.read_csv(\"customer_demographics_contaminated (1).csv\")      # Customer demographic information\n",
    "txn_raw    = pd.read_csv(\"customer_transactions_contaminated.csv\")          # Customer transaction history\n",
    "social_raw = pd.read_csv(\"social_media_interactions_contaminated (1).csv\")  # Social media interactions\n",
    "\n",
    "# Display the shape of each DataFrame (rows, columns) to confirm successful loading\n",
    "print(\"Demographics dataset shape:\", demo_raw.shape)\n",
    "print(\"Transactions dataset shape:\", txn_raw.shape)\n",
    "print(\"Social Media dataset shape:\", social_raw.shape)\n",
    "\n",
    "# Preview the first five rows of each dataset to examine their structure and values\n",
    "print(\"\\n=== DEMOGRAPHICS (first 5 rows) ===\")\n",
    "display(demo_raw.head())\n",
    "\n",
    "print(\"\\n=== TRANSACTIONS (first 5 rows) ===\")\n",
    "display(txn_raw.head())\n",
    "\n",
    "print(\"\\n=== SOCIAL MEDIA (first 5 rows) ===\")\n",
    "display(social_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464db5",
   "metadata": {},
   "source": [
    "## STEP 2 — Quick Checks (columns, types, missing values)\n",
    "This step examines the basic structure of each dataset (column names, data types, and missing values). These diagnostics inform the subsequent cleaning operations such as type parsing, imputation, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad06e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics columns: ['CustomerID', 'Age', 'Gender', 'Location', 'IncomeLevel', 'SignupDate']\n",
      "Transactions columns: ['CustomerID', 'TransactionID', 'TransactionDate', 'Amount', 'ProductCategory', 'PaymentMethod']\n",
      "Social columns: ['CustomerID', 'InteractionID', 'InteractionDate', 'Platform', 'InteractionType', 'Sentiment']\n",
      "\n",
      "Demographics dtypes:\n",
      " CustomerID     object\n",
      "Age            object\n",
      "Gender         object\n",
      "Location       object\n",
      "IncomeLevel    object\n",
      "SignupDate     object\n",
      "dtype: object\n",
      "\n",
      "Transactions dtypes:\n",
      " CustomerID         object\n",
      "TransactionID      object\n",
      "TransactionDate    object\n",
      "Amount             object\n",
      "ProductCategory    object\n",
      "PaymentMethod      object\n",
      "dtype: object\n",
      "\n",
      "Social dtypes:\n",
      " CustomerID         object\n",
      "InteractionID      object\n",
      "InteractionDate    object\n",
      "Platform           object\n",
      "InteractionType    object\n",
      "Sentiment          object\n",
      "dtype: object\n",
      "\n",
      "Missing values (Demographics):\n",
      " CustomerID       0\n",
      "Age            291\n",
      "Gender           0\n",
      "Location         0\n",
      "IncomeLevel    303\n",
      "SignupDate       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (Transactions):\n",
      " CustomerID           0\n",
      "TransactionID        0\n",
      "TransactionDate      0\n",
      "Amount             304\n",
      "ProductCategory    299\n",
      "PaymentMethod        0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (Social):\n",
      " CustomerID           0\n",
      "InteractionID        0\n",
      "InteractionDate      0\n",
      "Platform           311\n",
      "InteractionType      0\n",
      "Sentiment          329\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Structural checks (columns, types, missing values)\n",
    "\n",
    "# Column names\n",
    "print(\"Demographics columns:\", list(demo_raw.columns))\n",
    "print(\"Transactions columns:\", list(txn_raw.columns))\n",
    "print(\"Social columns:\", list(social_raw.columns))\n",
    "\n",
    "# Data types\n",
    "print(\"\\nDemographics dtypes:\\n\", demo_raw.dtypes)\n",
    "print(\"\\nTransactions dtypes:\\n\", txn_raw.dtypes)\n",
    "print(\"\\nSocial dtypes:\\n\", social_raw.dtypes)\n",
    "\n",
    "# Missing values per column\n",
    "print(\"\\nMissing values (Demographics):\\n\", demo_raw.isna().sum())\n",
    "print(\"\\nMissing values (Transactions):\\n\", txn_raw.isna().sum())\n",
    "print(\"\\nMissing values (Social):\\n\", social_raw.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3591d",
   "metadata": {},
   "source": [
    "## STEP 3 — Standardize Column Names\n",
    "Column names are standardized to a consistent snake_case convention. Consistent naming reduces the likelihood of downstream errors and simplifies reference in code and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2da1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'age', 'gender', 'location', 'income_level', 'signup_date']\n",
      "['customer_id', 'transaction_id', 'transaction_date', 'amount', 'product_category', 'payment_method']\n",
      "['customer_id', 'interaction_id', 'interaction_date', 'platform', 'interaction_type', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Standardize column names to snake_case\n",
    "\n",
    "# Create normalized copies\n",
    "demo = demo_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"Age\": \"age\",\n",
    "    \"Gender\": \"gender\",\n",
    "    \"Location\": \"location\",\n",
    "    \"IncomeLevel\": \"income_level\",\n",
    "    \"SignupDate\": \"signup_date\"\n",
    "})\n",
    "\n",
    "txn = txn_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"TransactionID\": \"transaction_id\",\n",
    "    \"TransactionDate\": \"transaction_date\",\n",
    "    \"Amount\": \"amount\",\n",
    "    \"ProductCategory\": \"product_category\",\n",
    "    \"PaymentMethod\": \"payment_method\"\n",
    "})\n",
    "\n",
    "social = social_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"InteractionID\": \"interaction_id\",\n",
    "    \"InteractionDate\": \"interaction_date\",\n",
    "    \"Platform\": \"platform\",\n",
    "    \"InteractionType\": \"interaction_type\",\n",
    "    \"Sentiment\": \"sentiment\"\n",
    "})\n",
    "\n",
    "# Verify standardized names\n",
    "print(list(demo.columns))\n",
    "print(list(txn.columns))\n",
    "print(list(social.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bd098",
   "metadata": {},
   "source": [
    "## STEP 4 — Clean Text Fields and Harmonize Null Tokens\n",
    "Text fields are trimmed to remove extraneous whitespace, and common null tokens (e.g., “NA”, “none”, “-”) are converted to NaN. This ensures that missingness is represented uniformly across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8006eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo dataset (first 10 rows after cleaning):\n",
      "                            customer_id   age  gender           location  \\\n",
      "0  9207fa75-5758-48d1-94ad-19c041e0520f  51.0  Female         Jensenberg   \n",
      "1  5fb09cd8-a473-46f7-80bd-6e49cf509078   NaN  Female       Castilloport   \n",
      "2  c139496e-cc89-498a-bd90-1fb4627b6cff  37.0    Male  Lake Jennifertown   \n",
      "3  50118139-7264-428f-81cc-a25fddc5d6dd  44.0    Male          Port Carl   \n",
      "4  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4  50.0  Female          Jessebury   \n",
      "5  2de49c7c-32ae-4ba8-b058-622a090d7094  53.0  Female         Emilyville   \n",
      "6  89f7de44-e592-43c6-b033-bcbcf24088ba   NaN  Female    South Derekbury   \n",
      "7  9f2128cd-2f2b-4f71-987c-29fd045021f0   NaN  Female          Lake Erin   \n",
      "8  fbcd7128-ce04-4799-8c1c-722330507b96   NaN  Female     New Reginabury   \n",
      "9  40f5a5dd-a46f-4a5e-8c0f-fe04389ddec8   NaN  Female          Mariaberg   \n",
      "\n",
      "  income_level signup_date  \n",
      "0          Low  2022-11-17  \n",
      "1         High  2020-07-21  \n",
      "2          NaN  2021-01-01  \n",
      "3       Medium  2024-06-10  \n",
      "4         High  2023-08-24  \n",
      "5          Low  2022-02-13  \n",
      "6         High  2019-12-08  \n",
      "7       Medium  2022-04-26  \n",
      "8          NaN  2022-04-17  \n",
      "9         High  2024-02-18  \n",
      "\n",
      "Transaction dataset (first 10 rows after cleaning):\n",
      "                            customer_id                        transaction_id  \\\n",
      "0  60567026-f719-4cd6-849e-137e86d8938f  5ff75116-0a50-4d04-80fb-31e5ccbb0769   \n",
      "1  4090ba85-b111-4f75-a792-c777965f5255  2c39b9fe-ff57-4d39-9321-9f5cdf187aa1   \n",
      "2  9223891b-73ff-4d5c-b8ae-13ece82ee28b  f79588dd-3db9-4ffa-97f8-7de0e64259f1   \n",
      "3  9243eebc-938f-480c-8564-16d503d250de  401c0fc9-60df-4455-ad78-67c132f9897d   \n",
      "4  6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f  2034aebc-8280-4254-a667-92bcd1c2be4f   \n",
      "5  3eccdcf9-e8ce-4e44-9f62-8bef88e70672  78dfb45e-16e0-4ecc-8f67-f2e210c31c3e   \n",
      "6  7fbc5847-a652-4fc8-a7d8-5e85d846e91b  591f1ea2-c97d-44dd-9539-71ac89b94ff3   \n",
      "7  958e5c8d-48ca-42dd-bb71-a766a374233a  fb24e098-3ab9-40a2-bcc3-b8ebb23f549a   \n",
      "8  39c6e7d2-6c4b-44c0-8961-ddc1ecbdb0c6  833b026a-7c02-4101-832d-62c07569b0f6   \n",
      "9  474f9233-0616-431b-b3e9-a6feabe68abb  f8bde21a-7a6b-41f6-ac39-eeb77640fa9b   \n",
      "\n",
      "  transaction_date  amount product_category payment_method  \n",
      "0       2024-05-15  117.64         Clothing         PayPal  \n",
      "1       2023-04-26  466.14  Health & Beauty  Bank Transfer  \n",
      "2       2022-09-23  563.99         Clothing     Debit Card  \n",
      "3       2024-04-15  254.44       Automotive         PayPal  \n",
      "4       2024-06-03  590.52    Home & Garden  Bank Transfer  \n",
      "5       2024-04-07    Free              NaN    Credit Card  \n",
      "6       2024-01-12     NaN       Automotive         PayPal  \n",
      "7       2023-03-10   399.7    Home & Garden         PayPal  \n",
      "8       2024-01-26  296.99         Clothing         PayPal  \n",
      "9       2023-06-15  149.39         Clothing     Debit Card  \n",
      "\n",
      "Social dataset (first 10 rows after cleaning):\n",
      "                            customer_id                        interaction_id  \\\n",
      "0  2dcb9523-356b-40b2-a67b-1f27797de261  e5d15761-d0a7-4329-89e3-79a892c56097   \n",
      "1  e12c37b3-7d4d-472f-9fd8-0df2cb3001aa  02f9f376-70ae-4fcd-9070-1db977939948   \n",
      "2  08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2  a83fa04c-f109-4f24-8ce1-2078154f6a1c   \n",
      "3  efdfdfc9-5dbb-4478-911a-101a390a0285  28a69c4b-a2e4-4c74-a130-1132d7733fdf   \n",
      "4  ca1e90f6-0e5f-492e-ab92-252ff540da18  d9d1c6f8-5e15-4738-b52b-13c2982420cc   \n",
      "5  3e44871b-f56c-4576-b1ca-d1dc999e2166  0c409883-8396-48e4-83fb-887329848696   \n",
      "6  aa5eea4b-c948-41f4-9285-229a470002aa  4034dadf-6541-40d6-a7f0-16b20a009c04   \n",
      "7  7d83304d-2501-4c9f-ba63-a1a14343e51f  bcfc43e7-c5aa-4dbd-9961-0e108784b199   \n",
      "8  bd90f5cb-05a7-40f4-acb1-eedf48b58ffa  d2a06664-703d-4bc1-9401-a06f8c43fda5   \n",
      "9  63ee220c-19ae-4113-b45c-276b22b068e1  67cdfb0b-3da6-46b8-a1f4-03b2abc81f58   \n",
      "\n",
      "  interaction_date   platform interaction_type sentiment  \n",
      "0       2023-07-11        NaN          Comment       NaN  \n",
      "1       2023-07-06    Twitter            Share       NaN  \n",
      "2       2024-05-24  Instagram          Comment   Neutral  \n",
      "3       2023-11-01  Instagram             Like   Neutral  \n",
      "4       2023-07-08  Instagram             Like       NaN  \n",
      "5       2023-12-18  Instagram          Comment  Positive  \n",
      "6       2023-11-15  Instagram            Share  Positive  \n",
      "7       2024-03-29  Instagram          Comment       NaN  \n",
      "8       2024-05-02        NaN             Like   Neutral  \n",
      "9       2023-11-18        NaN          Comment  Positive  \n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Trim whitespace and convert common null tokens to NaN\n",
    "\n",
    "# Convert common null-like tokens to real NaN so dropna will catch them\n",
    "for df in [demo, txn, social]:\n",
    "    for c in df.select_dtypes(include=\"object\").columns:\n",
    "        df[c] = df[c].replace([\"nan\", \"NaN\", \"Nan\", \"NULL\", \"None\", \"\"], np.nan)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define tokens that should be treated as missing values\n",
    "null_tokens = {\"\", \"na\", \"n/a\", \"none\", \"null\", \"nan\", \"-\", \"--\", \"unknown\", \"missing\"}\n",
    "\n",
    "# Apply to all object (string) columns in each dataset\n",
    "for df in [demo, txn, social]:\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        # Strip leading/trailing whitespace\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        # Replace listed tokens with NaN\n",
    "        df[col] = df[col].replace({t: np.nan for t in null_tokens})\n",
    "        # Convert literal strings \"nan\"/\"None\" to NaN\n",
    "        df[col] = df[col].replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "# Print first 10 rows after cleaning\n",
    "print(\"Demo dataset (first 10 rows after cleaning):\")\n",
    "print(demo.head(10))\n",
    "\n",
    "print(\"\\nTransaction dataset (first 10 rows after cleaning):\")\n",
    "print(txn.head(10))\n",
    "\n",
    "print(\"\\nSocial dataset (first 10 rows after cleaning):\")\n",
    "print(social.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c0f4d",
   "metadata": {},
   "source": [
    "## STEP 5 — Remove Exact Duplicate Rows\n",
    "Exact duplicate rows are removed to avoid double-counting and ensure the reliability of aggregations and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55164c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics duplicates removed: 177\n",
      "Transactions duplicates removed: 185\n",
      "Social duplicates removed: 180\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Drop exact duplicates\n",
    "\n",
    "demo_before = len(demo)\n",
    "demo = demo.drop_duplicates()\n",
    "print(\"Demographics duplicates removed:\", demo_before - len(demo))\n",
    "\n",
    "txn_before = len(txn)\n",
    "txn = txn.drop_duplicates()\n",
    "print(\"Transactions duplicates removed:\", txn_before - len(txn))\n",
    "\n",
    "social_before = len(social)\n",
    "social = social.drop_duplicates()\n",
    "print(\"Social duplicates removed:\", social_before - len(social))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d2fa1",
   "metadata": {},
   "source": [
    "## STEP 6 — Parse Dates and Coerce Numeric Amounts\n",
    "Date columns are converted into proper datetime format, and the amount column is coerced into numeric values. Using .loc ensures we avoid chained assignment warnings in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf583c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amount\n",
      "0  117.64\n",
      "1  466.14\n",
      "2  563.99\n",
      "3  254.44\n",
      "4  590.52\n",
      "\n",
      "First 10 signup_date entries:\n",
      "0    2022-11-17 00:00:00\n",
      "1    2020-07-21 00:00:00\n",
      "2    2021-01-01 00:00:00\n",
      "3    2024-06-10 00:00:00\n",
      "4    2023-08-24 00:00:00\n",
      "5    2022-02-13 00:00:00\n",
      "6    2019-12-08 00:00:00\n",
      "7    2022-04-26 00:00:00\n",
      "8    2022-04-17 00:00:00\n",
      "9    2024-02-18 00:00:00\n",
      "Name: signup_date, dtype: object\n",
      "\n",
      "First 10 transaction_date entries:\n",
      "0    2024-05-15 00:00:00\n",
      "1    2023-04-26 00:00:00\n",
      "2    2022-09-23 00:00:00\n",
      "3    2024-04-15 00:00:00\n",
      "4    2024-06-03 00:00:00\n",
      "5    2024-04-07 00:00:00\n",
      "6    2024-01-12 00:00:00\n",
      "7    2023-03-10 00:00:00\n",
      "8    2024-01-26 00:00:00\n",
      "9    2023-06-15 00:00:00\n",
      "Name: transaction_date, dtype: object\n",
      "\n",
      "First 10 interaction_date entries:\n",
      "0    2023-07-11 00:00:00\n",
      "1    2023-07-06 00:00:00\n",
      "2    2024-05-24 00:00:00\n",
      "3    2023-11-01 00:00:00\n",
      "4    2023-07-08 00:00:00\n",
      "5    2023-12-18 00:00:00\n",
      "6    2023-11-15 00:00:00\n",
      "7    2024-03-29 00:00:00\n",
      "8    2024-05-02 00:00:00\n",
      "9    2023-11-18 00:00:00\n",
      "Name: interaction_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Parse dates and convert amount to numeric (revised)\n",
    "\n",
    "# Convert date-like columns safely using .loc\n",
    "if \"signup_date\" in demo.columns:\n",
    "    demo.loc[:, \"signup_date\"] = pd.to_datetime(demo[\"signup_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"transaction_date\" in txn.columns:\n",
    "    txn.loc[:, \"transaction_date\"] = pd.to_datetime(txn[\"transaction_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"interaction_date\" in social.columns:\n",
    "    social.loc[:, \"interaction_date\"] = pd.to_datetime(social[\"interaction_date\"], errors=\"coerce\")\n",
    "\n",
    "# Safely handle the 'amount' column\n",
    "if \"amount\" in txn.columns:\n",
    "    # Step 1: work on a temporary string series\n",
    "    amt_str = txn[\"amount\"].astype(str)\n",
    "    # Step 2: remove currency symbols, spaces, and commas\n",
    "    amt_str = (amt_str.str.replace(\"$\", \"\", regex=False)\n",
    "                        .str.replace(\"₱\", \"\", regex=False)\n",
    "                        .str.replace(\",\", \"\", regex=False)\n",
    "                        .str.replace(\" \", \"\", regex=False))\n",
    "    # Step 3: convert cleaned strings to numeric\n",
    "    txn.loc[:, \"amount\"] = pd.to_numeric(amt_str, errors=\"coerce\")\n",
    "\n",
    "# Quick verification\n",
    "print(txn[[\"amount\"]].head())\n",
    "\n",
    "# Print first 10 entries of datetime columns\n",
    "if \"signup_date\" in demo.columns:\n",
    "    print(\"\\nFirst 10 signup_date entries:\")\n",
    "    print(demo[\"signup_date\"].head(10))\n",
    "\n",
    "if \"transaction_date\" in txn.columns:\n",
    "    print(\"\\nFirst 10 transaction_date entries:\")\n",
    "    print(txn[\"transaction_date\"].head(10))\n",
    "\n",
    "if \"interaction_date\" in social.columns:\n",
    "    print(\"\\nFirst 10 interaction_date entries:\")\n",
    "    print(social[\"interaction_date\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce10040",
   "metadata": {},
   "source": [
    "## STEP 7 — Normalize Categorical Values\n",
    "Categorical values (e.g., gender, payment method, product category, platform, sentiment) are normalized to consistent labels. Assignments use .loc to avoid chained-assignment warnings and ensure changes apply to the intended DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "627ea916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defensive copy to avoid any view-related warnings\n",
    "demo   = demo.copy()\n",
    "txn    = txn.copy()\n",
    "social = social.copy()\n",
    "\n",
    "# --- Gender (demographics) ---\n",
    "def normalize_gender(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"m\", \"male\"]:\n",
    "        return \"Male\"\n",
    "    if s in [\"f\", \"female\"]:\n",
    "        return \"Female\"\n",
    "    if s in [\"other\", \"nonbinary\", \"non-binary\", \"nb\"]:\n",
    "        return \"Other\"\n",
    "    return str(x).title()\n",
    "\n",
    "if \"gender\" in demo.columns:\n",
    "    demo.loc[:, \"gender\"] = demo[\"gender\"].map(normalize_gender)\n",
    "\n",
    "# --- Payment method & product category (transactions) ---\n",
    "if \"payment_method\" in txn.columns:\n",
    "    txn.loc[:, \"payment_method\"] = (\n",
    "        txn[\"payment_method\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "if \"product_category\" in txn.columns:\n",
    "    txn.loc[:, \"product_category\"] = (\n",
    "        txn[\"product_category\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "# --- Platform & interaction type (social) ---\n",
    "if \"platform\" in social.columns:\n",
    "    social.loc[:, \"platform\"] = (\n",
    "        social[\"platform\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "if \"interaction_type\" in social.columns:\n",
    "    social.loc[:, \"interaction_type\"] = (\n",
    "        social[\"interaction_type\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "# --- Sentiment (social) ---\n",
    "def normalize_sentiment(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"pos\", \"positive\", \"1\", \"+\", \"good\"]:\n",
    "        return \"Positive\"\n",
    "    if s in [\"neu\", \"neutral\", \"0\", \"meh\"]:\n",
    "        return \"Neutral\"\n",
    "    if s in [\"neg\", \"negative\", \"-1\", \"-\", \"bad\"]:\n",
    "        return \"Negative\"\n",
    "    return str(x).title()\n",
    "\n",
    "if \"sentiment\" in social.columns:\n",
    "    social.loc[:, \"sentiment\"] = social[\"sentiment\"].map(normalize_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb85ad",
   "metadata": {},
   "source": [
    "## STEP 8 — Handle Missing Values\n",
    "Missing values are addressed by dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a863923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs (Demographics):\n",
      " customer_id     0\n",
      "age             0\n",
      "gender          0\n",
      "location        0\n",
      "income_level    0\n",
      "signup_date     0\n",
      "dtype: int64\n",
      "\n",
      "Remaining NaNs (Transactions):\n",
      " customer_id         0\n",
      "transaction_id      0\n",
      "transaction_date    0\n",
      "amount              0\n",
      "product_category    0\n",
      "payment_method      0\n",
      "is_refund           0\n",
      "dtype: int64\n",
      "\n",
      "Remaining NaNs (Social):\n",
      " customer_id         0\n",
      "interaction_id      0\n",
      "interaction_date    0\n",
      "platform            0\n",
      "interaction_type    0\n",
      "sentiment           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make sure numeric columns are coerced properly\n",
    "if \"age\" in demo.columns:\n",
    "    demo[\"age\"] = pd.to_numeric(demo[\"age\"], errors=\"coerce\")\n",
    "\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"amount\"] = pd.to_numeric(txn[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing essential fields\n",
    "demo = demo.dropna(subset=[\"customer_id\", \"age\", \"gender\", \"location\"])\n",
    "txn = txn.dropna(subset=[\"customer_id\", \"transaction_id\", \"amount\"])\n",
    "social = social.dropna(subset=[\"customer_id\", \"interaction_id\", \"interaction_date\"])\n",
    "\n",
    "# Recreate refund flag\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"is_refund\"] = txn[\"amount\"] < 0\n",
    "    \n",
    "demo = demo.dropna().copy()\n",
    "txn = txn.dropna().copy()\n",
    "social = social.dropna().copy()\n",
    "\n",
    "# Recreate refund flag (in case df changed)\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"is_refund\"] = txn[\"amount\"] < 0\n",
    "\n",
    "# Re-check\n",
    "print(\"Remaining NaNs (Demographics):\\n\", demo.isna().sum())\n",
    "print(\"\\nRemaining NaNs (Transactions):\\n\", txn.isna().sum())\n",
    "print(\"\\nRemaining NaNs (Social):\\n\", social.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e85cf",
   "metadata": {},
   "source": [
    "## STEP 9 — Mitigate Outliers via Light Clipping\n",
    "Extreme outliers in numeric fields can distort statistical summaries and models. Here, values are clipped to the 1st and 99th percentiles for age and amount to reduce undue influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1b4e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo: dropped 51 rows with invalid ages\n",
      "Txn: dropped 39 rows with negative amounts\n"
     ]
    }
   ],
   "source": [
    "# STEP 9 — Drop Outliers (Abnormal Values)\n",
    "\n",
    "# DEMOGRAPHICS: drop unrealistic ages (<0 or >120)\n",
    "if \"age\" in demo.columns:\n",
    "    before = len(demo)\n",
    "    demo = demo[(demo[\"age\"] >= 0) & (demo[\"age\"] <= 120)]\n",
    "    print(f\"Demo: dropped {before - len(demo)} rows with invalid ages\")\n",
    "\n",
    "# TRANSACTIONS: drop negative amounts (if refunds not required in this intro task)\n",
    "if \"amount\" in txn.columns:\n",
    "    before = len(txn)\n",
    "    txn = txn[txn[\"amount\"] >= 0]\n",
    "    print(f\"Txn: dropped {before - len(txn)} rows with negative amounts\")\n",
    "\n",
    "# SOCIAL: drop negative interaction counts if any (defensive)\n",
    "for col in [\"likes\",\"comments\",\"shares\",\"score\"]:\n",
    "    if col in social.columns:\n",
    "        before = len(social)\n",
    "        social = social[social[col].isna() | (social[col] >= 0)]\n",
    "        print(f\"Social: dropped {before - len(social)} rows with invalid {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d91785-ecb5-4c1f-83ac-1ee1fc99877f",
   "metadata": {},
   "source": [
    "### STEP 9.1 - Outlier Analysis (Bulk Purchases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3c102-6d58-4623-8015-ba738bad5edb",
   "metadata": {},
   "source": [
    "Outlier analysis of the contaminated transactions dataset showed that several purchase amounts exceeded the IQR-based upper bound, identifying them as unrealistic bulk purchases that required correction during cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70033221-38b3-4ace-a599-d2ad428a608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bulk Purchase Outlier Analysis (Contaminated Data) ===\n",
      "            stat        value\n",
      "0          count  2866.000000\n",
      "1           mean   490.916479\n",
      "2         median   497.285000\n",
      "3             Q1   228.240000\n",
      "4             Q3   746.935000\n",
      "5            IQR   518.695000\n",
      "6    lower_bound  -549.802500\n",
      "7    upper_bound  1524.977500\n",
      "8          p97_5   980.853750\n",
      "9   iqr_outliers     0.000000\n",
      "10    tail_count    72.000000\n"
     ]
    }
   ],
   "source": [
    "# === OUTLIER ANALYSIS ON CONTAMINATED TRANSACTIONS ===\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CONTAMINATED dataset\n",
    "txc = pd.read_csv(\"customer_transactions_contaminated.csv\")\n",
    "\n",
    "# Make column names lowercase for consistency\n",
    "txc.columns = [c.strip().lower() for c in txc.columns]\n",
    "\n",
    "# Coerce Amount to numeric (to handle invalid or string entries in contaminated data)\n",
    "txc[\"amount_num\"] = pd.to_numeric(txc[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Drop missing values\n",
    "s = txc[\"amount_num\"].dropna()\n",
    "\n",
    "# Compute IQR stats\n",
    "q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "\n",
    "# High-spend tail (top 2.5%)\n",
    "p975 = s.quantile(0.975)\n",
    "\n",
    "# Flag outliers\n",
    "txc[\"is_iqr_outlier\"] = (txc[\"amount_num\"] < lower) | (txc[\"amount_num\"] > upper)\n",
    "txc[\"is_top_tail\"] = txc[\"amount_num\"] >= p975\n",
    "\n",
    "# Summary statistics\n",
    "summary_df = pd.DataFrame({\n",
    "    \"stat\": [\"count\",\"mean\",\"median\",\"Q1\",\"Q3\",\"IQR\",\"lower_bound\",\"upper_bound\",\"p97_5\",\"iqr_outliers\",\"tail_count\"],\n",
    "    \"value\": [\n",
    "        int(s.count()), float(s.mean()), float(s.median()), float(q1), float(q3), float(iqr),\n",
    "        float(lower), float(upper), float(p975),\n",
    "        int(txc[\"is_iqr_outlier\"].sum()), int(txc[\"is_top_tail\"].sum())\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Bulk Purchase Outlier Analysis (Contaminated Data) ===\")\n",
    "print(summary_df)\n",
    "\n",
    "# Save for appendix\n",
    "summary_df.to_csv(\"amount_iqr_summary_contaminated.csv\", index=False)\n",
    "txc.loc[txc[\"is_iqr_outlier\"] | txc[\"is_top_tail\"]]\\\n",
    "   .sort_values(\"amount_num\", ascending=False).head(25)\\\n",
    "   .to_csv(\"amount_extreme_examples_contaminated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99c3ca",
   "metadata": {},
   "source": [
    "## STEP 10 — Enforce Identifier Uniqueness\n",
    "Primary identifiers are enforced as unique within their respective tables. This step removes any duplicated keys to maintain entity integrity and prevent ambiguity in joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "968cfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate customer_id removed (demo): 0\n",
      "Duplicate transaction_id removed (txn): 0\n",
      "Duplicate interaction_id removed (social): 0\n"
     ]
    }
   ],
   "source": [
    "if \"customer_id\" in demo.columns:\n",
    "    before = len(demo)\n",
    "    demo = demo.drop_duplicates(subset=[\"customer_id\"])\n",
    "    print(\"Duplicate customer_id removed (demo):\", before - len(demo))\n",
    "\n",
    "if \"transaction_id\" in txn.columns:\n",
    "    before = len(txn)\n",
    "    txn = txn.drop_duplicates(subset=[\"transaction_id\"])\n",
    "    print(\"Duplicate transaction_id removed (txn):\", before - len(txn))\n",
    "\n",
    "if \"interaction_id\" in social.columns:\n",
    "    before = len(social)\n",
    "    social = social.drop_duplicates(subset=[\"interaction_id\"])\n",
    "    print(\"Duplicate interaction_id removed (social):\", before - len(social))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366462b4",
   "metadata": {},
   "source": [
    "## STEP 11 — Persist the Cleaned Outputs\n",
    "Finally, the cleaned datasets and the integrated master table are exported to CSV files. Persisting these outputs enables reproducibility and reuse in analytics and BI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd32d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the following files:\n",
      " - cleaned_customer_demographics.csv\n",
      " - cleaned_customer_transactions.csv\n",
      " - cleaned_social_media_interactions.csv\n"
     ]
    }
   ],
   "source": [
    "demo.to_csv(\"cleaned_customer_demographics.csv\", index=False)\n",
    "txn.to_csv(\"cleaned_customer_transactions.csv\", index=False)\n",
    "social.to_csv(\"cleaned_social_media_interactions.csv\", index=False)\n",
    "\n",
    "print(\"Saved the following files:\")\n",
    "print(\" - cleaned_customer_demographics.csv\")\n",
    "print(\" - cleaned_customer_transactions.csv\")\n",
    "print(\" - cleaned_social_media_interactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602eae83-d8c7-485f-967f-dd19276e12f8",
   "metadata": {},
   "source": [
    "## STEP 12 - Merging the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b2cacf6-c6e0-46f6-a26c-52155666d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions: (2573, 7)\n",
      "Demographics: (2338, 6)\n",
      "Social: (2613, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tx = pd.read_csv(\"cleaned_customer_transactions.csv\")\n",
    "demo = pd.read_csv(\"cleaned_customer_demographics.csv\")\n",
    "soc = pd.read_csv(\"cleaned_social_media_interactions.csv\")\n",
    "\n",
    "# Standardize IDs\n",
    "for df in (tx, demo, soc):\n",
    "    df[\"customer_id\"] = df[\"customer_id\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Transactions:\", tx.shape)\n",
    "print(\"Demographics:\", demo.shape)\n",
    "print(\"Social:\", soc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c633c033-1efc-4c94-9fa9-0d1669b0233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Social: (1745, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interactions_count</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "      <th>Share</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Very Negative</th>\n",
       "      <th>Very Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0009fdd2-ae63-45ca-8d5b-d0ea98381f7b</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000c6bbd-533a-432d-922c-ab64197e71c5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00145374-004a-4685-af4d-c8a8967b969e</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00c15eff-8bcf-4d12-a5d4-992e45e3309f</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00c60c9c-6cb7-4259-84c2-7bbe5da2bf87</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      interactions_count  Comment  Like  \\\n",
       "customer_id                                                               \n",
       "0009fdd2-ae63-45ca-8d5b-d0ea98381f7b                   2        0     0   \n",
       "000c6bbd-533a-432d-922c-ab64197e71c5                   4        1     3   \n",
       "00145374-004a-4685-af4d-c8a8967b969e                   2        0     2   \n",
       "00c15eff-8bcf-4d12-a5d4-992e45e3309f                   2        1     0   \n",
       "00c60c9c-6cb7-4259-84c2-7bbe5da2bf87                   1        1     0   \n",
       "\n",
       "                                      Share  Negative  Neutral  Positive  \\\n",
       "customer_id                                                                \n",
       "0009fdd2-ae63-45ca-8d5b-d0ea98381f7b      2         0        1         1   \n",
       "000c6bbd-533a-432d-922c-ab64197e71c5      0         1        1         2   \n",
       "00145374-004a-4685-af4d-c8a8967b969e      0         1        1         0   \n",
       "00c15eff-8bcf-4d12-a5d4-992e45e3309f      1         1        0         1   \n",
       "00c60c9c-6cb7-4259-84c2-7bbe5da2bf87      0         0        0         1   \n",
       "\n",
       "                                      Very Negative  Very Positive  \n",
       "customer_id                                                         \n",
       "0009fdd2-ae63-45ca-8d5b-d0ea98381f7b              0              0  \n",
       "000c6bbd-533a-432d-922c-ab64197e71c5              0              0  \n",
       "00145374-004a-4685-af4d-c8a8967b969e              0              0  \n",
       "00c15eff-8bcf-4d12-a5d4-992e45e3309f              0              0  \n",
       "00c60c9c-6cb7-4259-84c2-7bbe5da2bf87              0              0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count interactions per type and sentiment per customer\n",
    "type_counts = pd.pivot_table(\n",
    "    soc.assign(count=1),\n",
    "    index=\"customer_id\", columns=\"interaction_type\",\n",
    "    values=\"count\", aggfunc=\"sum\", fill_value=0\n",
    ")\n",
    "\n",
    "sent_counts = pd.pivot_table(\n",
    "    soc.assign(count=1),\n",
    "    index=\"customer_id\", columns=\"sentiment\",\n",
    "    values=\"count\", aggfunc=\"sum\", fill_value=0\n",
    ")\n",
    "\n",
    "soc_basic = soc.groupby(\"customer_id\").agg(\n",
    "    interactions_count=(\"interaction_id\", \"nunique\")\n",
    ")\n",
    "\n",
    "soc_agg = soc_basic.join(type_counts, how=\"left\").join(sent_counts, how=\"left\").fillna(0)\n",
    "print(\"Aggregated Social:\", soc_agg.shape)\n",
    "soc_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ecc067b-6cc0-4714-8c25-244056ef571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction-level enriched dataset saved: (2573, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>amount</th>\n",
       "      <th>product_category</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>is_refund</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>...</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>interactions_count</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "      <th>Share</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Very Negative</th>\n",
       "      <th>Very Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60567026-f719-4cd6-849e-137e86d8938f</td>\n",
       "      <td>5ff75116-0a50-4d04-80fb-31e5ccbb0769</td>\n",
       "      <td>2024-05-15 00:00:00</td>\n",
       "      <td>117.64</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Paypal</td>\n",
       "      <td>False</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Eric</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-03-09 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4090ba85-b111-4f75-a792-c777965f5255</td>\n",
       "      <td>2c39b9fe-ff57-4d39-9321-9f5cdf187aa1</td>\n",
       "      <td>2023-04-26 00:00:00</td>\n",
       "      <td>466.14</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Bank Transfer</td>\n",
       "      <td>False</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>West Scotttown</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-02-11 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9223891b-73ff-4d5c-b8ae-13ece82ee28b</td>\n",
       "      <td>f79588dd-3db9-4ffa-97f8-7de0e64259f1</td>\n",
       "      <td>2022-09-23 00:00:00</td>\n",
       "      <td>563.99</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>False</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Sherriburgh</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-04-22 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9243eebc-938f-480c-8564-16d503d250de</td>\n",
       "      <td>401c0fc9-60df-4455-ad78-67c132f9897d</td>\n",
       "      <td>2024-04-15 00:00:00</td>\n",
       "      <td>254.44</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>Paypal</td>\n",
       "      <td>False</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Gomezview</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-08-05 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f</td>\n",
       "      <td>2034aebc-8280-4254-a667-92bcd1c2be4f</td>\n",
       "      <td>2024-06-03 00:00:00</td>\n",
       "      <td>590.52</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>Bank Transfer</td>\n",
       "      <td>False</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Nicoleview</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-11-15 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            customer_id                        transaction_id  \\\n",
       "0  60567026-f719-4cd6-849e-137e86d8938f  5ff75116-0a50-4d04-80fb-31e5ccbb0769   \n",
       "1  4090ba85-b111-4f75-a792-c777965f5255  2c39b9fe-ff57-4d39-9321-9f5cdf187aa1   \n",
       "2  9223891b-73ff-4d5c-b8ae-13ece82ee28b  f79588dd-3db9-4ffa-97f8-7de0e64259f1   \n",
       "3  9243eebc-938f-480c-8564-16d503d250de  401c0fc9-60df-4455-ad78-67c132f9897d   \n",
       "4  6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f  2034aebc-8280-4254-a667-92bcd1c2be4f   \n",
       "\n",
       "      transaction_date  amount product_category payment_method  is_refund  \\\n",
       "0  2024-05-15 00:00:00  117.64         Clothing         Paypal      False   \n",
       "1  2023-04-26 00:00:00  466.14  Health & Beauty  Bank Transfer      False   \n",
       "2  2022-09-23 00:00:00  563.99         Clothing     Debit Card      False   \n",
       "3  2024-04-15 00:00:00  254.44       Automotive         Paypal      False   \n",
       "4  2024-06-03 00:00:00  590.52    Home & Garden  Bank Transfer      False   \n",
       "\n",
       "    age  gender        location  ...          signup_date interactions_count  \\\n",
       "0  37.0    Male       Port Eric  ...  2022-03-09 00:00:00                NaN   \n",
       "1  23.0    Male  West Scotttown  ...  2021-02-11 00:00:00                NaN   \n",
       "2  46.0  Female     Sherriburgh  ...  2022-04-22 00:00:00                1.0   \n",
       "3  40.0  Female       Gomezview  ...  2021-08-05 00:00:00                1.0   \n",
       "4  41.0  Female      Nicoleview  ...  2021-11-15 00:00:00                1.0   \n",
       "\n",
       "   Comment  Like  Share  Negative  Neutral  Positive  Very Negative  \\\n",
       "0      NaN   NaN    NaN       NaN      NaN       NaN            NaN   \n",
       "1      NaN   NaN    NaN       NaN      NaN       NaN            NaN   \n",
       "2      0.0   1.0    0.0       0.0      1.0       0.0            0.0   \n",
       "3      0.0   0.0    1.0       0.0      1.0       0.0            0.0   \n",
       "4      0.0   0.0    1.0       1.0      0.0       0.0            0.0   \n",
       "\n",
       "   Very Positive  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row = transaction + demographics + aggregated social\n",
    "tx_enriched = (\n",
    "    tx.merge(demo, on=\"customer_id\", how=\"left\", suffixes=(\"\", \"_demo\"))\n",
    "      .merge(soc_agg, on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "tx_enriched.to_csv(\"merged_transactions_enriched.csv\", index=False)\n",
    "print(\"Transaction-level enriched dataset saved:\", tx_enriched.shape)\n",
    "tx_enriched.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75046bfb-acdd-46a4-b8cb-e42352849a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer-level dataset saved: (2338, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>income_level</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>n_transactions</th>\n",
       "      <th>total_spend</th>\n",
       "      <th>avg_basket_value</th>\n",
       "      <th>max_spend</th>\n",
       "      <th>min_spend</th>\n",
       "      <th>interactions_count</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "      <th>Share</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Very Negative</th>\n",
       "      <th>Very Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9207fa75-5758-48d1-94ad-19c041e0520f</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jensenberg</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-11-17 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>660.87</td>\n",
       "      <td>660.87</td>\n",
       "      <td>660.87</td>\n",
       "      <td>660.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50118139-7264-428f-81cc-a25fddc5d6dd</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Carl</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2024-06-10 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jessebury</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-08-24 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>733.46</td>\n",
       "      <td>733.46</td>\n",
       "      <td>733.46</td>\n",
       "      <td>733.46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2de49c7c-32ae-4ba8-b058-622a090d7094</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Emilyville</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-02-13 00:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>891.74</td>\n",
       "      <td>445.87</td>\n",
       "      <td>615.28</td>\n",
       "      <td>276.46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8602d631-457c-49c1-8b59-8efb2a4448d4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Keithville</td>\n",
       "      <td>High</td>\n",
       "      <td>2022-04-17 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>815.21</td>\n",
       "      <td>815.21</td>\n",
       "      <td>815.21</td>\n",
       "      <td>815.21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            customer_id   age  gender         location  \\\n",
       "0  9207fa75-5758-48d1-94ad-19c041e0520f  51.0  Female       Jensenberg   \n",
       "1  50118139-7264-428f-81cc-a25fddc5d6dd  44.0    Male        Port Carl   \n",
       "2  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4  50.0  Female        Jessebury   \n",
       "3  2de49c7c-32ae-4ba8-b058-622a090d7094  53.0  Female       Emilyville   \n",
       "4  8602d631-457c-49c1-8b59-8efb2a4448d4  51.0    Male  East Keithville   \n",
       "\n",
       "  income_level          signup_date  n_transactions  total_spend  \\\n",
       "0          Low  2022-11-17 00:00:00             1.0       660.87   \n",
       "1       Medium  2024-06-10 00:00:00             NaN          NaN   \n",
       "2         High  2023-08-24 00:00:00             1.0       733.46   \n",
       "3          Low  2022-02-13 00:00:00             2.0       891.74   \n",
       "4         High  2022-04-17 00:00:00             1.0       815.21   \n",
       "\n",
       "   avg_basket_value  max_spend  min_spend  interactions_count  Comment  Like  \\\n",
       "0            660.87     660.87     660.87                 2.0      1.0   0.0   \n",
       "1               NaN        NaN        NaN                 2.0      0.0   1.0   \n",
       "2            733.46     733.46     733.46                 2.0      0.0   1.0   \n",
       "3            445.87     615.28     276.46                 3.0      1.0   0.0   \n",
       "4            815.21     815.21     815.21                 2.0      1.0   0.0   \n",
       "\n",
       "   Share  Negative  Neutral  Positive  Very Negative  Very Positive  \n",
       "0    1.0       1.0      0.0       1.0            0.0            0.0  \n",
       "1    1.0       1.0      1.0       0.0            0.0            0.0  \n",
       "2    1.0       1.0      0.0       1.0            0.0            0.0  \n",
       "3    2.0       0.0      0.0       3.0            0.0            0.0  \n",
       "4    1.0       0.0      1.0       1.0            0.0            0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate transactions per customer\n",
    "cust_tx_agg = tx.groupby(\"customer_id\").agg(\n",
    "    n_transactions=(\"transaction_id\",\"nunique\"),\n",
    "    total_spend=(\"amount\",\"sum\"),\n",
    "    avg_basket_value=(\"amount\",\"mean\"),\n",
    "    max_spend=(\"amount\",\"max\"),\n",
    "    min_spend=(\"amount\",\"min\")\n",
    ")\n",
    "\n",
    "# Merge demographics + transactions + social\n",
    "customer_level = demo.merge(cust_tx_agg, on=\"customer_id\", how=\"left\")\\\n",
    "                     .merge(soc_agg, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "customer_level.to_csv(\"merged_customer_level.csv\", index=False)\n",
    "print(\"Customer-level dataset saved:\", customer_level.shape)\n",
    "customer_level.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
