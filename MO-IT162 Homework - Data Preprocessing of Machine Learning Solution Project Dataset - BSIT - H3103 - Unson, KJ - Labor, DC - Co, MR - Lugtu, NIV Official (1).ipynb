{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a38f5f0",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Machine Learning Solution Project Dataset\n",
    "\n",
    "This notebook cleans and standardizes **three related datasets**:\n",
    "\n",
    "- **Demographics** (`customer_demographics_contaminated (1).csv`): CustomerID, Age, Gender, Location, IncomeLevel, SignupDate\n",
    "- **Transactions** (`customer_transactions_contaminated.csv`): CustomerID, TransactionID, TransactionDate, Amount, ProductCategory, PaymentMethod\n",
    "- **Social Media Interactions** (`social_media_interactions_contaminated (1).csv`): CustomerID, InteractionID, InteractionDate, Platform, InteractionType, Sentiment\n",
    "\n",
    "You'll find each step explained in **Markdown** (why the step matters) followed by **executable Python code** with comments.\n",
    "\n",
    "**High-level goals:**  \n",
    "- Ensure consistent column names and data types  \n",
    "- Handle missing values, duplicates, outliers  \n",
    "- Normalize categories (e.g., gender, platforms, payment methods)  \n",
    "- Parse dates and numeric amounts robustly  \n",
    "- Save **cleaned** CSVs plus a **joined master** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565aaed",
   "metadata": {},
   "source": [
    "## STEP 1 — Loading the CSVs\n",
    "In this step, the three raw CSV datasets are imported into pandas DataFrames. Displaying their shapes and initial rows allows us to verify that the files were successfully loaded and to obtain an initial view of their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc720fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics dataset shape: (3200, 6)\n",
      "Transactions dataset shape: (3200, 6)\n",
      "Social Media dataset shape: (3200, 6)\n",
      "\n",
      "=== DEMOGRAPHICS (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Location</th>\n",
       "      <th>IncomeLevel</th>\n",
       "      <th>SignupDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9207fa75-5758-48d1-94ad-19c041e0520f</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jensenberg</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-11-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fb09cd8-a473-46f7-80bd-6e49cf509078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>Castilloport</td>\n",
       "      <td>High</td>\n",
       "      <td>2020-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c139496e-cc89-498a-bd90-1fb4627b6cff</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Lake Jennifertown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50118139-7264-428f-81cc-a25fddc5d6dd</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Carl</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2024-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jessebury</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-08-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID   Age  Gender           Location  \\\n",
       "0  9207fa75-5758-48d1-94ad-19c041e0520f  51.0  Female         Jensenberg   \n",
       "1  5fb09cd8-a473-46f7-80bd-6e49cf509078   NaN  Female       Castilloport   \n",
       "2  c139496e-cc89-498a-bd90-1fb4627b6cff  37.0    Male  Lake Jennifertown   \n",
       "3  50118139-7264-428f-81cc-a25fddc5d6dd  44.0    Male          Port Carl   \n",
       "4  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4  50.0  Female          Jessebury   \n",
       "\n",
       "  IncomeLevel  SignupDate  \n",
       "0         Low  2022-11-17  \n",
       "1        High  2020-07-21  \n",
       "2         NaN  2021-01-01  \n",
       "3      Medium  2024-06-10  \n",
       "4        High  2023-08-24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSACTIONS (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>Amount</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>PaymentMethod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60567026-f719-4cd6-849e-137e86d8938f</td>\n",
       "      <td>5ff75116-0a50-4d04-80fb-31e5ccbb0769</td>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>117.64</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>PayPal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4090ba85-b111-4f75-a792-c777965f5255</td>\n",
       "      <td>2c39b9fe-ff57-4d39-9321-9f5cdf187aa1</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>466.14</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9223891b-73ff-4d5c-b8ae-13ece82ee28b</td>\n",
       "      <td>f79588dd-3db9-4ffa-97f8-7de0e64259f1</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>563.99</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>Debit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9243eebc-938f-480c-8564-16d503d250de</td>\n",
       "      <td>401c0fc9-60df-4455-ad78-67c132f9897d</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>254.44</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>PayPal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f</td>\n",
       "      <td>2034aebc-8280-4254-a667-92bcd1c2be4f</td>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>590.52</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID                         TransactionID  \\\n",
       "0  60567026-f719-4cd6-849e-137e86d8938f  5ff75116-0a50-4d04-80fb-31e5ccbb0769   \n",
       "1  4090ba85-b111-4f75-a792-c777965f5255  2c39b9fe-ff57-4d39-9321-9f5cdf187aa1   \n",
       "2  9223891b-73ff-4d5c-b8ae-13ece82ee28b  f79588dd-3db9-4ffa-97f8-7de0e64259f1   \n",
       "3  9243eebc-938f-480c-8564-16d503d250de  401c0fc9-60df-4455-ad78-67c132f9897d   \n",
       "4  6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f  2034aebc-8280-4254-a667-92bcd1c2be4f   \n",
       "\n",
       "  TransactionDate  Amount  ProductCategory  PaymentMethod  \n",
       "0      2024-05-15  117.64         Clothing         PayPal  \n",
       "1      2023-04-26  466.14  Health & Beauty  Bank Transfer  \n",
       "2      2022-09-23  563.99         Clothing     Debit Card  \n",
       "3      2024-04-15  254.44       Automotive         PayPal  \n",
       "4      2024-06-03  590.52    Home & Garden  Bank Transfer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SOCIAL MEDIA (first 5 rows) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>InteractionID</th>\n",
       "      <th>InteractionDate</th>\n",
       "      <th>Platform</th>\n",
       "      <th>InteractionType</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2dcb9523-356b-40b2-a67b-1f27797de261</td>\n",
       "      <td>e5d15761-d0a7-4329-89e3-79a892c56097</td>\n",
       "      <td>2023-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comment</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e12c37b3-7d4d-472f-9fd8-0df2cb3001aa</td>\n",
       "      <td>02f9f376-70ae-4fcd-9070-1db977939948</td>\n",
       "      <td>2023-07-06</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Share</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2</td>\n",
       "      <td>a83fa04c-f109-4f24-8ce1-2078154f6a1c</td>\n",
       "      <td>2024-05-24</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efdfdfc9-5dbb-4478-911a-101a390a0285</td>\n",
       "      <td>28a69c4b-a2e4-4c74-a130-1132d7733fdf</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Like</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca1e90f6-0e5f-492e-ab92-252ff540da18</td>\n",
       "      <td>d9d1c6f8-5e15-4738-b52b-13c2982420cc</td>\n",
       "      <td>2023-07-08</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Like</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             CustomerID                         InteractionID  \\\n",
       "0  2dcb9523-356b-40b2-a67b-1f27797de261  e5d15761-d0a7-4329-89e3-79a892c56097   \n",
       "1  e12c37b3-7d4d-472f-9fd8-0df2cb3001aa  02f9f376-70ae-4fcd-9070-1db977939948   \n",
       "2  08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2  a83fa04c-f109-4f24-8ce1-2078154f6a1c   \n",
       "3  efdfdfc9-5dbb-4478-911a-101a390a0285  28a69c4b-a2e4-4c74-a130-1132d7733fdf   \n",
       "4  ca1e90f6-0e5f-492e-ab92-252ff540da18  d9d1c6f8-5e15-4738-b52b-13c2982420cc   \n",
       "\n",
       "  InteractionDate   Platform InteractionType Sentiment  \n",
       "0      2023-07-11        NaN         Comment       NaN  \n",
       "1      2023-07-06    Twitter           Share       NaN  \n",
       "2      2024-05-24  Instagram         Comment   Neutral  \n",
       "3      2023-11-01  Instagram            Like   Neutral  \n",
       "4      2023-07-08  Instagram            Like       NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Load the CSV files into pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load each dataset into a DataFrame\n",
    "demo_raw   = pd.read_csv(\"customer_demographics_contaminated (1).csv\")      # Customer demographic information\n",
    "txn_raw    = pd.read_csv(\"customer_transactions_contaminated.csv\")          # Customer transaction history\n",
    "social_raw = pd.read_csv(\"social_media_interactions_contaminated (1).csv\")  # Social media interactions\n",
    "\n",
    "# Display the shape of each DataFrame (rows, columns) to confirm successful loading\n",
    "print(\"Demographics dataset shape:\", demo_raw.shape)\n",
    "print(\"Transactions dataset shape:\", txn_raw.shape)\n",
    "print(\"Social Media dataset shape:\", social_raw.shape)\n",
    "\n",
    "# Preview the first five rows of each dataset to examine their structure and values\n",
    "print(\"\\n=== DEMOGRAPHICS (first 5 rows) ===\")\n",
    "display(demo_raw.head())\n",
    "\n",
    "print(\"\\n=== TRANSACTIONS (first 5 rows) ===\")\n",
    "display(txn_raw.head())\n",
    "\n",
    "print(\"\\n=== SOCIAL MEDIA (first 5 rows) ===\")\n",
    "display(social_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464db5",
   "metadata": {},
   "source": [
    "## STEP 2 — Quick Checks (columns, types, missing values)\n",
    "This step examines the basic structure of each dataset (column names, data types, and missing values). These diagnostics inform the subsequent cleaning operations such as type parsing, imputation, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad06e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics columns: ['CustomerID', 'Age', 'Gender', 'Location', 'IncomeLevel', 'SignupDate']\n",
      "Transactions columns: ['CustomerID', 'TransactionID', 'TransactionDate', 'Amount', 'ProductCategory', 'PaymentMethod']\n",
      "Social columns: ['CustomerID', 'InteractionID', 'InteractionDate', 'Platform', 'InteractionType', 'Sentiment']\n",
      "\n",
      "Demographics dtypes:\n",
      " CustomerID     object\n",
      "Age            object\n",
      "Gender         object\n",
      "Location       object\n",
      "IncomeLevel    object\n",
      "SignupDate     object\n",
      "dtype: object\n",
      "\n",
      "Transactions dtypes:\n",
      " CustomerID         object\n",
      "TransactionID      object\n",
      "TransactionDate    object\n",
      "Amount             object\n",
      "ProductCategory    object\n",
      "PaymentMethod      object\n",
      "dtype: object\n",
      "\n",
      "Social dtypes:\n",
      " CustomerID         object\n",
      "InteractionID      object\n",
      "InteractionDate    object\n",
      "Platform           object\n",
      "InteractionType    object\n",
      "Sentiment          object\n",
      "dtype: object\n",
      "\n",
      "Missing values (Demographics):\n",
      " CustomerID       0\n",
      "Age            291\n",
      "Gender           0\n",
      "Location         0\n",
      "IncomeLevel    303\n",
      "SignupDate       0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (Transactions):\n",
      " CustomerID           0\n",
      "TransactionID        0\n",
      "TransactionDate      0\n",
      "Amount             304\n",
      "ProductCategory    299\n",
      "PaymentMethod        0\n",
      "dtype: int64\n",
      "\n",
      "Missing values (Social):\n",
      " CustomerID           0\n",
      "InteractionID        0\n",
      "InteractionDate      0\n",
      "Platform           311\n",
      "InteractionType      0\n",
      "Sentiment          329\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Structural checks (columns, types, missing values)\n",
    "\n",
    "# Column names\n",
    "print(\"Demographics columns:\", list(demo_raw.columns))\n",
    "print(\"Transactions columns:\", list(txn_raw.columns))\n",
    "print(\"Social columns:\", list(social_raw.columns))\n",
    "\n",
    "# Data types\n",
    "print(\"\\nDemographics dtypes:\\n\", demo_raw.dtypes)\n",
    "print(\"\\nTransactions dtypes:\\n\", txn_raw.dtypes)\n",
    "print(\"\\nSocial dtypes:\\n\", social_raw.dtypes)\n",
    "\n",
    "# Missing values per column\n",
    "print(\"\\nMissing values (Demographics):\\n\", demo_raw.isna().sum())\n",
    "print(\"\\nMissing values (Transactions):\\n\", txn_raw.isna().sum())\n",
    "print(\"\\nMissing values (Social):\\n\", social_raw.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3591d",
   "metadata": {},
   "source": [
    "## STEP 3 — Standardize Column Names\n",
    "Column names are standardized to a consistent snake_case convention. Consistent naming reduces the likelihood of downstream errors and simplifies reference in code and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2da1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'age', 'gender', 'location', 'income_level', 'signup_date']\n",
      "['customer_id', 'transaction_id', 'transaction_date', 'amount', 'product_category', 'payment_method']\n",
      "['customer_id', 'interaction_id', 'interaction_date', 'platform', 'interaction_type', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Standardize column names to snake_case\n",
    "\n",
    "# Create normalized copies\n",
    "demo = demo_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"Age\": \"age\",\n",
    "    \"Gender\": \"gender\",\n",
    "    \"Location\": \"location\",\n",
    "    \"IncomeLevel\": \"income_level\",\n",
    "    \"SignupDate\": \"signup_date\"\n",
    "})\n",
    "\n",
    "txn = txn_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"TransactionID\": \"transaction_id\",\n",
    "    \"TransactionDate\": \"transaction_date\",\n",
    "    \"Amount\": \"amount\",\n",
    "    \"ProductCategory\": \"product_category\",\n",
    "    \"PaymentMethod\": \"payment_method\"\n",
    "})\n",
    "\n",
    "social = social_raw.rename(columns={\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"InteractionID\": \"interaction_id\",\n",
    "    \"InteractionDate\": \"interaction_date\",\n",
    "    \"Platform\": \"platform\",\n",
    "    \"InteractionType\": \"interaction_type\",\n",
    "    \"Sentiment\": \"sentiment\"\n",
    "})\n",
    "\n",
    "# Verify standardized names\n",
    "print(list(demo.columns))\n",
    "print(list(txn.columns))\n",
    "print(list(social.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bd098",
   "metadata": {},
   "source": [
    "## STEP 4 — Clean Text Fields and Harmonize Null Tokens\n",
    "Text fields are trimmed to remove extraneous whitespace, and common null tokens (e.g., “NA”, “none”, “-”) are converted to NaN. This ensures that missingness is represented uniformly across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8006eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo dataset (first 10 rows after cleaning):\n",
      "                             customer_id    age  gender            location  \\\n",
      "0   9207fa75-5758-48d1-94ad-19c041e0520f   51.0  Female          Jensenberg   \n",
      "3   50118139-7264-428f-81cc-a25fddc5d6dd   44.0    Male           Port Carl   \n",
      "4   7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4   50.0  Female           Jessebury   \n",
      "5   2de49c7c-32ae-4ba8-b058-622a090d7094   53.0  Female          Emilyville   \n",
      "10  c96a5ee9-f1a6-416a-adc6-1c8b128c7399  150.0    Male          Hansontown   \n",
      "13  8602d631-457c-49c1-8b59-8efb2a4448d4   51.0    Male     East Keithville   \n",
      "15  56f11a95-76f1-4a97-b38f-db1dc95da1ed   59.0  Female         East Nathan   \n",
      "16  3f520998-2bc4-4f38-af82-5ab2de339984   59.0  Female           Smithside   \n",
      "17  b2f4c25e-be11-4912-9d14-5c288616e56e   29.0    Male  South Timothyhaven   \n",
      "18  8de7560f-370d-4cfd-8135-20a3de237264   31.0  Female         Colemanstad   \n",
      "\n",
      "   income_level signup_date  \n",
      "0           Low  2022-11-17  \n",
      "3        Medium  2024-06-10  \n",
      "4          High  2023-08-24  \n",
      "5           Low  2022-02-13  \n",
      "10         High  2020-09-17  \n",
      "13         High  2022-04-17  \n",
      "15       Medium  2020-01-02  \n",
      "16         High  2023-10-06  \n",
      "17         High  2023-07-09  \n",
      "18          Low  2020-12-01  \n",
      "\n",
      "Transaction dataset (first 10 rows after cleaning):\n",
      "                             customer_id  \\\n",
      "0   60567026-f719-4cd6-849e-137e86d8938f   \n",
      "1   4090ba85-b111-4f75-a792-c777965f5255   \n",
      "2   9223891b-73ff-4d5c-b8ae-13ece82ee28b   \n",
      "3   9243eebc-938f-480c-8564-16d503d250de   \n",
      "4   6e3e8eb8-bc0f-4ffe-9f74-5d5efec9502f   \n",
      "7   958e5c8d-48ca-42dd-bb71-a766a374233a   \n",
      "8   39c6e7d2-6c4b-44c0-8961-ddc1ecbdb0c6   \n",
      "9   474f9233-0616-431b-b3e9-a6feabe68abb   \n",
      "10  187ec535-ad79-418a-aa08-86fd8a2becbb   \n",
      "11  848f7178-b94b-4d8e-8baa-6a3886828887   \n",
      "\n",
      "                          transaction_id transaction_date  amount  \\\n",
      "0   5ff75116-0a50-4d04-80fb-31e5ccbb0769       2024-05-15  117.64   \n",
      "1   2c39b9fe-ff57-4d39-9321-9f5cdf187aa1       2023-04-26  466.14   \n",
      "2   f79588dd-3db9-4ffa-97f8-7de0e64259f1       2022-09-23  563.99   \n",
      "3   401c0fc9-60df-4455-ad78-67c132f9897d       2024-04-15  254.44   \n",
      "4   2034aebc-8280-4254-a667-92bcd1c2be4f       2024-06-03  590.52   \n",
      "7   fb24e098-3ab9-40a2-bcc3-b8ebb23f549a       2023-03-10  399.70   \n",
      "8   833b026a-7c02-4101-832d-62c07569b0f6       2024-01-26  296.99   \n",
      "9   f8bde21a-7a6b-41f6-ac39-eeb77640fa9b       2023-06-15  149.39   \n",
      "10  85dc5e9e-1bd7-44e3-9a13-980736fedd3c       2023-06-01  255.41   \n",
      "11  80cd99c4-01c7-47ec-8272-79250c1c4725       2024-03-09  813.81   \n",
      "\n",
      "   product_category payment_method  is_refund  \n",
      "0          Clothing         Paypal      False  \n",
      "1   Health & Beauty  Bank Transfer      False  \n",
      "2          Clothing     Debit Card      False  \n",
      "3        Automotive         Paypal      False  \n",
      "4     Home & Garden  Bank Transfer      False  \n",
      "7     Home & Garden         Paypal      False  \n",
      "8          Clothing         Paypal      False  \n",
      "9          Clothing     Debit Card      False  \n",
      "10  Health & Beauty  Bank Transfer      False  \n",
      "11      Electronics    Credit Card      False  \n",
      "\n",
      "Social dataset (first 10 rows after cleaning):\n",
      "                             customer_id  \\\n",
      "2   08a911a3-65e6-4f5d-a6a1-ae7ddcbe28a2   \n",
      "3   efdfdfc9-5dbb-4478-911a-101a390a0285   \n",
      "5   3e44871b-f56c-4576-b1ca-d1dc999e2166   \n",
      "6   aa5eea4b-c948-41f4-9285-229a470002aa   \n",
      "8   bd90f5cb-05a7-40f4-acb1-eedf48b58ffa   \n",
      "9   63ee220c-19ae-4113-b45c-276b22b068e1   \n",
      "10  481bcc62-9dcb-4766-bbdd-79f5554d73f8   \n",
      "11  9c16dd71-a1f0-4801-8f00-860c2ee154a1   \n",
      "12  e8d8dccc-96b4-4451-ac1b-c4de6d18c3dd   \n",
      "14  aac57e67-f138-426a-8f26-c4620e29f4e6   \n",
      "\n",
      "                          interaction_id interaction_date   platform  \\\n",
      "2   a83fa04c-f109-4f24-8ce1-2078154f6a1c       2024-05-24  Instagram   \n",
      "3   28a69c4b-a2e4-4c74-a130-1132d7733fdf       2023-11-01  Instagram   \n",
      "5   0c409883-8396-48e4-83fb-887329848696       2023-12-18  Instagram   \n",
      "6   4034dadf-6541-40d6-a7f0-16b20a009c04       2023-11-15  Instagram   \n",
      "8   d2a06664-703d-4bc1-9401-a06f8c43fda5       2024-05-02        NaN   \n",
      "9   67cdfb0b-3da6-46b8-a1f4-03b2abc81f58       2023-11-18        NaN   \n",
      "10  785b7854-4548-4251-83f2-a25da78bfe40       2024-05-28    Twitter   \n",
      "11  be6a0be7-84a8-428e-8526-13c3500e35e9       2024-06-27  Instagram   \n",
      "12  26e650e5-a8af-45ea-9c49-3569a7d1a2fe       2023-08-31   Facebook   \n",
      "14  4fcac03b-1081-49b9-a101-6fb70fa0b7d4       2024-06-10  Instagram   \n",
      "\n",
      "   interaction_type sentiment  \n",
      "2           Comment   Neutral  \n",
      "3              Like   Neutral  \n",
      "5           Comment  Positive  \n",
      "6             Share  Positive  \n",
      "8              Like   Neutral  \n",
      "9           Comment  Positive  \n",
      "10             Like  Positive  \n",
      "11          Comment   Neutral  \n",
      "12          Comment  Negative  \n",
      "14          Comment   Neutral  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_20076\\3487238919.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df[c].replace([\"nan\", \"NaN\", \"Nan\", \"NULL\", \"None\", \"\"], np.nan)\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_20076\\3487238919.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df[c].replace([\"nan\", \"NaN\", \"Nan\", \"NULL\", \"None\", \"\"], np.nan)\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_20076\\3487238919.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[c] = df[c].replace([\"nan\", \"NaN\", \"Nan\", \"NULL\", \"None\", \"\"], np.nan)\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Trim whitespace and convert common null tokens to NaN\n",
    "\n",
    "# Convert common null-like tokens to real NaN so dropna will catch them\n",
    "for df in [demo, txn, social]:\n",
    "    for c in df.select_dtypes(include=\"object\").columns:\n",
    "        df[c] = df[c].replace([\"nan\", \"NaN\", \"Nan\", \"NULL\", \"None\", \"\"], np.nan)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define tokens that should be treated as missing values\n",
    "null_tokens = {\"\", \"na\", \"n/a\", \"none\", \"null\", \"nan\", \"-\", \"--\", \"unknown\", \"missing\"}\n",
    "\n",
    "# Apply to all object (string) columns in each dataset\n",
    "for df in [demo, txn, social]:\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        # Strip leading/trailing whitespace\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        # Replace listed tokens with NaN\n",
    "        df[col] = df[col].replace({t: np.nan for t in null_tokens})\n",
    "        # Convert literal strings \"nan\"/\"None\" to NaN\n",
    "        df[col] = df[col].replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "# Print first 10 rows after cleaning\n",
    "print(\"Demo dataset (first 10 rows after cleaning):\")\n",
    "print(demo.head(10))\n",
    "\n",
    "print(\"\\nTransaction dataset (first 10 rows after cleaning):\")\n",
    "print(txn.head(10))\n",
    "\n",
    "print(\"\\nSocial dataset (first 10 rows after cleaning):\")\n",
    "print(social.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c0f4d",
   "metadata": {},
   "source": [
    "## STEP 5 — Remove Exact Duplicate Rows\n",
    "Exact duplicate rows are removed to avoid double-counting and ensure the reliability of aggregations and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55164c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics duplicates removed: 0\n",
      "Transactions duplicates removed: 0\n",
      "Social duplicates removed: 0\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Drop exact duplicates\n",
    "\n",
    "demo_before = len(demo)\n",
    "demo = demo.drop_duplicates()\n",
    "print(\"Demographics duplicates removed:\", demo_before - len(demo))\n",
    "\n",
    "txn_before = len(txn)\n",
    "txn = txn.drop_duplicates()\n",
    "print(\"Transactions duplicates removed:\", txn_before - len(txn))\n",
    "\n",
    "social_before = len(social)\n",
    "social = social.drop_duplicates()\n",
    "print(\"Social duplicates removed:\", social_before - len(social))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d2fa1",
   "metadata": {},
   "source": [
    "## STEP 6 — Parse Dates and Coerce Numeric Amounts\n",
    "Date columns are converted into proper datetime format, and the amount column is coerced into numeric values. Using .loc ensures we avoid chained assignment warnings in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf583c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amount\n",
      "0  117.64\n",
      "1  466.14\n",
      "2  563.99\n",
      "3  254.44\n",
      "4  590.52\n",
      "\n",
      "First 10 signup_date entries:\n",
      "0    2022-11-17 00:00:00\n",
      "1    2020-07-21 00:00:00\n",
      "2    2021-01-01 00:00:00\n",
      "3    2024-06-10 00:00:00\n",
      "4    2023-08-24 00:00:00\n",
      "5    2022-02-13 00:00:00\n",
      "6    2019-12-08 00:00:00\n",
      "7    2022-04-26 00:00:00\n",
      "8    2022-04-17 00:00:00\n",
      "9    2024-02-18 00:00:00\n",
      "Name: signup_date, dtype: object\n",
      "\n",
      "First 10 transaction_date entries:\n",
      "0    2024-05-15 00:00:00\n",
      "1    2023-04-26 00:00:00\n",
      "2    2022-09-23 00:00:00\n",
      "3    2024-04-15 00:00:00\n",
      "4    2024-06-03 00:00:00\n",
      "5    2024-04-07 00:00:00\n",
      "6    2024-01-12 00:00:00\n",
      "7    2023-03-10 00:00:00\n",
      "8    2024-01-26 00:00:00\n",
      "9    2023-06-15 00:00:00\n",
      "Name: transaction_date, dtype: object\n",
      "\n",
      "First 10 interaction_date entries:\n",
      "0    2023-07-11 00:00:00\n",
      "1    2023-07-06 00:00:00\n",
      "2    2024-05-24 00:00:00\n",
      "3    2023-11-01 00:00:00\n",
      "4    2023-07-08 00:00:00\n",
      "5    2023-12-18 00:00:00\n",
      "6    2023-11-15 00:00:00\n",
      "7    2024-03-29 00:00:00\n",
      "8    2024-05-02 00:00:00\n",
      "9    2023-11-18 00:00:00\n",
      "Name: interaction_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Parse dates and convert amount to numeric (revised)\n",
    "\n",
    "# Convert date-like columns safely using .loc\n",
    "if \"signup_date\" in demo.columns:\n",
    "    demo.loc[:, \"signup_date\"] = pd.to_datetime(demo[\"signup_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"transaction_date\" in txn.columns:\n",
    "    txn.loc[:, \"transaction_date\"] = pd.to_datetime(txn[\"transaction_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"interaction_date\" in social.columns:\n",
    "    social.loc[:, \"interaction_date\"] = pd.to_datetime(social[\"interaction_date\"], errors=\"coerce\")\n",
    "\n",
    "# Safely handle the 'amount' column\n",
    "if \"amount\" in txn.columns:\n",
    "    # Step 1: work on a temporary string series\n",
    "    amt_str = txn[\"amount\"].astype(str)\n",
    "    # Step 2: remove currency symbols, spaces, and commas\n",
    "    amt_str = (amt_str.str.replace(\"$\", \"\", regex=False)\n",
    "                        .str.replace(\"₱\", \"\", regex=False)\n",
    "                        .str.replace(\",\", \"\", regex=False)\n",
    "                        .str.replace(\" \", \"\", regex=False))\n",
    "    # Step 3: convert cleaned strings to numeric\n",
    "    txn.loc[:, \"amount\"] = pd.to_numeric(amt_str, errors=\"coerce\")\n",
    "\n",
    "# Quick verification\n",
    "print(txn[[\"amount\"]].head())\n",
    "\n",
    "# Print first 10 entries of datetime columns\n",
    "if \"signup_date\" in demo.columns:\n",
    "    print(\"\\nFirst 10 signup_date entries:\")\n",
    "    print(demo[\"signup_date\"].head(10))\n",
    "\n",
    "if \"transaction_date\" in txn.columns:\n",
    "    print(\"\\nFirst 10 transaction_date entries:\")\n",
    "    print(txn[\"transaction_date\"].head(10))\n",
    "\n",
    "if \"interaction_date\" in social.columns:\n",
    "    print(\"\\nFirst 10 interaction_date entries:\")\n",
    "    print(social[\"interaction_date\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce10040",
   "metadata": {},
   "source": [
    "## STEP 7 — Normalize Categorical Values\n",
    "Categorical values (e.g., gender, payment method, product category, platform, sentiment) are normalized to consistent labels. Assignments use .loc to avoid chained-assignment warnings and ensure changes apply to the intended DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627ea916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defensive copy to avoid any view-related warnings\n",
    "demo   = demo.copy()\n",
    "txn    = txn.copy()\n",
    "social = social.copy()\n",
    "\n",
    "# --- Gender (demographics) ---\n",
    "def normalize_gender(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"m\", \"male\"]:\n",
    "        return \"Male\"\n",
    "    if s in [\"f\", \"female\"]:\n",
    "        return \"Female\"\n",
    "    if s in [\"other\", \"nonbinary\", \"non-binary\", \"nb\"]:\n",
    "        return \"Other\"\n",
    "    return str(x).title()\n",
    "\n",
    "if \"gender\" in demo.columns:\n",
    "    demo.loc[:, \"gender\"] = demo[\"gender\"].map(normalize_gender)\n",
    "\n",
    "# --- Payment method & product category (transactions) ---\n",
    "if \"payment_method\" in txn.columns:\n",
    "    txn.loc[:, \"payment_method\"] = (\n",
    "        txn[\"payment_method\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "if \"product_category\" in txn.columns:\n",
    "    txn.loc[:, \"product_category\"] = (\n",
    "        txn[\"product_category\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "# --- Platform & interaction type (social) ---\n",
    "if \"platform\" in social.columns:\n",
    "    social.loc[:, \"platform\"] = (\n",
    "        social[\"platform\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "if \"interaction_type\" in social.columns:\n",
    "    social.loc[:, \"interaction_type\"] = (\n",
    "        social[\"interaction_type\"].astype(str).str.strip().str.title()\n",
    "    )\n",
    "\n",
    "# --- Sentiment (social) ---\n",
    "def normalize_sentiment(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"pos\", \"positive\", \"1\", \"+\", \"good\"]:\n",
    "        return \"Positive\"\n",
    "    if s in [\"neu\", \"neutral\", \"0\", \"meh\"]:\n",
    "        return \"Neutral\"\n",
    "    if s in [\"neg\", \"negative\", \"-1\", \"-\", \"bad\"]:\n",
    "        return \"Negative\"\n",
    "    return str(x).title()\n",
    "\n",
    "if \"sentiment\" in social.columns:\n",
    "    social.loc[:, \"sentiment\"] = social[\"sentiment\"].map(normalize_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb85ad",
   "metadata": {},
   "source": [
    "## STEP 8 — Handle Missing Values\n",
    "Missing values are addressed by dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a863923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining NaNs (Demographics):\n",
      " customer_id     0\n",
      "age             0\n",
      "gender          0\n",
      "location        0\n",
      "income_level    0\n",
      "signup_date     0\n",
      "dtype: int64\n",
      "\n",
      "Remaining NaNs (Transactions):\n",
      " customer_id         0\n",
      "transaction_id      0\n",
      "transaction_date    0\n",
      "amount              0\n",
      "product_category    0\n",
      "payment_method      0\n",
      "is_refund           0\n",
      "dtype: int64\n",
      "\n",
      "Remaining NaNs (Social):\n",
      " customer_id         0\n",
      "interaction_id      0\n",
      "interaction_date    0\n",
      "platform            0\n",
      "interaction_type    0\n",
      "sentiment           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make sure numeric columns are coerced properly\n",
    "if \"age\" in demo.columns:\n",
    "    demo[\"age\"] = pd.to_numeric(demo[\"age\"], errors=\"coerce\")\n",
    "\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"amount\"] = pd.to_numeric(txn[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing essential fields\n",
    "demo = demo.dropna(subset=[\"customer_id\", \"age\", \"gender\", \"location\"])\n",
    "txn = txn.dropna(subset=[\"customer_id\", \"transaction_id\", \"amount\"])\n",
    "social = social.dropna(subset=[\"customer_id\", \"interaction_id\", \"interaction_date\"])\n",
    "\n",
    "# Recreate refund flag\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"is_refund\"] = txn[\"amount\"] < 0\n",
    "    \n",
    "demo = demo.dropna().copy()\n",
    "txn = txn.dropna().copy()\n",
    "social = social.dropna().copy()\n",
    "\n",
    "# Recreate refund flag (in case df changed)\n",
    "if \"amount\" in txn.columns:\n",
    "    txn[\"is_refund\"] = txn[\"amount\"] < 0\n",
    "\n",
    "# Re-check\n",
    "print(\"Remaining NaNs (Demographics):\\n\", demo.isna().sum())\n",
    "print(\"\\nRemaining NaNs (Transactions):\\n\", txn.isna().sum())\n",
    "print(\"\\nRemaining NaNs (Social):\\n\", social.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e85cf",
   "metadata": {},
   "source": [
    "## STEP 9 — Mitigate Outliers via Light Clipping\n",
    "Extreme outliers in numeric fields can distort statistical summaries and models. Here, values are clipped to the 1st and 99th percentiles for age and amount to reduce undue influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b4e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo: dropped 48 rows with invalid ages\n",
      "Txn: dropped 34 rows with negative amounts\n"
     ]
    }
   ],
   "source": [
    "# STEP 9 — Drop Outliers (Abnormal Values)\n",
    "\n",
    "# DEMOGRAPHICS: drop unrealistic ages (<0 or >120)\n",
    "if \"age\" in demo.columns:\n",
    "    before = len(demo)\n",
    "    demo = demo[(demo[\"age\"] >= 0) & (demo[\"age\"] <= 120)]\n",
    "    print(f\"Demo: dropped {before - len(demo)} rows with invalid ages\")\n",
    "\n",
    "# TRANSACTIONS: drop negative amounts (if refunds not required in this intro task)\n",
    "if \"amount\" in txn.columns:\n",
    "    before = len(txn)\n",
    "    txn = txn[txn[\"amount\"] >= 0]\n",
    "    print(f\"Txn: dropped {before - len(txn)} rows with negative amounts\")\n",
    "\n",
    "# SOCIAL: drop negative interaction counts if any (defensive)\n",
    "for col in [\"likes\",\"comments\",\"shares\",\"score\"]:\n",
    "    if col in social.columns:\n",
    "        before = len(social)\n",
    "        social = social[social[col].isna() | (social[col] >= 0)]\n",
    "        print(f\"Social: dropped {before - len(social)} rows with invalid {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99c3ca",
   "metadata": {},
   "source": [
    "## STEP 10 — Enforce Identifier Uniqueness\n",
    "Primary identifiers are enforced as unique within their respective tables. This step removes any duplicated keys to maintain entity integrity and prevent ambiguity in joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "968cfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate customer_id removed (demo): 0\n",
      "Duplicate transaction_id removed (txn): 0\n",
      "Duplicate interaction_id removed (social): 0\n"
     ]
    }
   ],
   "source": [
    "if \"customer_id\" in demo.columns:\n",
    "    before = len(demo)\n",
    "    demo = demo.drop_duplicates(subset=[\"customer_id\"])\n",
    "    print(\"Duplicate customer_id removed (demo):\", before - len(demo))\n",
    "\n",
    "if \"transaction_id\" in txn.columns:\n",
    "    before = len(txn)\n",
    "    txn = txn.drop_duplicates(subset=[\"transaction_id\"])\n",
    "    print(\"Duplicate transaction_id removed (txn):\", before - len(txn))\n",
    "\n",
    "if \"interaction_id\" in social.columns:\n",
    "    before = len(social)\n",
    "    social = social.drop_duplicates(subset=[\"interaction_id\"])\n",
    "    print(\"Duplicate interaction_id removed (social):\", before - len(social))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42e877",
   "metadata": {},
   "source": [
    "## STEP 11 — Build a Customer-Level Master Table\n",
    "A customer-level master table is constructed by left-joining demographic records with aggregated transaction and social metrics. This unified view facilitates downstream analysis, reporting, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70291a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master dataset shape: (2335, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>income_level</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>total_spend</th>\n",
       "      <th>avg_spend</th>\n",
       "      <th>txn_count</th>\n",
       "      <th>refund_count</th>\n",
       "      <th>interactions</th>\n",
       "      <th>platforms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9207fa75-5758-48d1-94ad-19c041e0520f</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jensenberg</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>660.87</td>\n",
       "      <td>660.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50118139-7264-428f-81cc-a25fddc5d6dd</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Carl</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2024-06-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jessebury</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-08-24</td>\n",
       "      <td>733.46</td>\n",
       "      <td>733.46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2de49c7c-32ae-4ba8-b058-622a090d7094</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Emilyville</td>\n",
       "      <td>Low</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>891.74</td>\n",
       "      <td>445.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8602d631-457c-49c1-8b59-8efb2a4448d4</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Keithville</td>\n",
       "      <td>High</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>815.21</td>\n",
       "      <td>815.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56f11a95-76f1-4a97-b38f-db1dc95da1ed</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>East Nathan</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3f520998-2bc4-4f38-af82-5ab2de339984</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Smithside</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-10-06</td>\n",
       "      <td>183.91</td>\n",
       "      <td>183.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b2f4c25e-be11-4912-9d14-5c288616e56e</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>South Timothyhaven</td>\n",
       "      <td>High</td>\n",
       "      <td>2023-07-09</td>\n",
       "      <td>533.94</td>\n",
       "      <td>533.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8de7560f-370d-4cfd-8135-20a3de237264</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Colemanstad</td>\n",
       "      <td>Low</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fcb513ca-6a74-4745-b855-6cc3e891cc02</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>East Stuart</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2022-01-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            customer_id   age  gender            location  \\\n",
       "0  9207fa75-5758-48d1-94ad-19c041e0520f  51.0  Female          Jensenberg   \n",
       "1  50118139-7264-428f-81cc-a25fddc5d6dd  44.0    Male           Port Carl   \n",
       "2  7d1f2bbc-8d16-4fbc-9b37-ece3324e8ed4  50.0  Female           Jessebury   \n",
       "3  2de49c7c-32ae-4ba8-b058-622a090d7094  53.0  Female          Emilyville   \n",
       "4  8602d631-457c-49c1-8b59-8efb2a4448d4  51.0    Male     East Keithville   \n",
       "5  56f11a95-76f1-4a97-b38f-db1dc95da1ed  59.0  Female         East Nathan   \n",
       "6  3f520998-2bc4-4f38-af82-5ab2de339984  59.0  Female           Smithside   \n",
       "7  b2f4c25e-be11-4912-9d14-5c288616e56e  29.0    Male  South Timothyhaven   \n",
       "8  8de7560f-370d-4cfd-8135-20a3de237264  31.0  Female         Colemanstad   \n",
       "9  fcb513ca-6a74-4745-b855-6cc3e891cc02  53.0  Female         East Stuart   \n",
       "\n",
       "  income_level signup_date  total_spend  avg_spend  txn_count  refund_count  \\\n",
       "0          Low  2022-11-17       660.87     660.87        1.0           0.0   \n",
       "1       Medium  2024-06-10          NaN        NaN        NaN           NaN   \n",
       "2         High  2023-08-24       733.46     733.46        1.0           0.0   \n",
       "3          Low  2022-02-13       891.74     445.87        2.0           0.0   \n",
       "4         High  2022-04-17       815.21     815.21        1.0           0.0   \n",
       "5       Medium  2020-01-02          NaN        NaN        NaN           NaN   \n",
       "6         High  2023-10-06       183.91     183.91        1.0           0.0   \n",
       "7         High  2023-07-09       533.94     533.94        1.0           0.0   \n",
       "8          Low  2020-12-01          NaN        NaN        NaN           NaN   \n",
       "9       Medium  2022-01-09          NaN        NaN        NaN           NaN   \n",
       "\n",
       "   interactions  platforms  \n",
       "0           2.0        2.0  \n",
       "1           2.0        2.0  \n",
       "2           2.0        2.0  \n",
       "3           3.0        2.0  \n",
       "4           2.0        2.0  \n",
       "5           3.0        2.0  \n",
       "6           NaN        NaN  \n",
       "7           1.0        1.0  \n",
       "8           NaN        NaN  \n",
       "9           NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregate transactions per customer\n",
    "if set([\"customer_id\", \"amount\", \"transaction_id\"]).issubset(txn.columns):\n",
    "    txn_agg = (txn.groupby(\"customer_id\")\n",
    "                 .agg(total_spend=(\"amount\", \"sum\"),\n",
    "                      avg_spend=(\"amount\", \"mean\"),\n",
    "                      txn_count=(\"transaction_id\", \"nunique\"),\n",
    "                      refund_count=(\"is_refund\", \"sum\"))\n",
    "                 .reset_index())\n",
    "else:\n",
    "    txn_agg = pd.DataFrame(columns=[\"customer_id\", \"total_spend\", \"avg_spend\", \"txn_count\", \"refund_count\"])\n",
    "\n",
    "# Aggregate social interactions per customer\n",
    "if set([\"customer_id\", \"interaction_id\"]).issubset(social.columns):\n",
    "    if \"platform\" in social.columns:\n",
    "        social_agg = (social.groupby(\"customer_id\")\n",
    "                        .agg(interactions=(\"interaction_id\", \"nunique\"),\n",
    "                             platforms=(\"platform\", \"nunique\"))\n",
    "                        .reset_index())\n",
    "    else:\n",
    "        social_agg = (social.groupby(\"customer_id\")\n",
    "                        .agg(interactions=(\"interaction_id\", \"nunique\"))\n",
    "                        .reset_index())\n",
    "else:\n",
    "    social_agg = pd.DataFrame(columns=[\"customer_id\", \"interactions\", \"platforms\"])\n",
    "\n",
    "# Start from demographics (one row per customer), then left-join aggregates\n",
    "master = demo.copy()\n",
    "if not txn_agg.empty:\n",
    "    master = master.merge(txn_agg, on=\"customer_id\", how=\"left\")\n",
    "if not social_agg.empty:\n",
    "    master = master.merge(social_agg, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "print(\"Master dataset shape:\", master.shape)\n",
    "display(master.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366462b4",
   "metadata": {},
   "source": [
    "## STEP 12 — Persist the Cleaned Outputs\n",
    "Finally, the cleaned datasets and the integrated master table are exported to CSV files. Persisting these outputs enables reproducibility and reuse in analytics and BI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bd32d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the following files:\n",
      " - cleaned_customer_demographics.csv\n",
      " - cleaned_customer_transactions.csv\n",
      " - cleaned_social_media_interactions.csv\n",
      " - cleaned_master.csv\n"
     ]
    }
   ],
   "source": [
    "demo.to_csv(\"cleaned_customer_demographics.csv\", index=False)\n",
    "txn.to_csv(\"cleaned_customer_transactions.csv\", index=False)\n",
    "social.to_csv(\"cleaned_social_media_interactions.csv\", index=False)\n",
    "master.to_csv(\"cleaned_master.csv\", index=False)\n",
    "\n",
    "print(\"Saved the following files:\")\n",
    "print(\" - cleaned_customer_demographics.csv\")\n",
    "print(\" - cleaned_customer_transactions.csv\")\n",
    "print(\" - cleaned_social_media_interactions.csv\")\n",
    "print(\" - cleaned_master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5d634-0a25-430d-8288-cd645344e86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
